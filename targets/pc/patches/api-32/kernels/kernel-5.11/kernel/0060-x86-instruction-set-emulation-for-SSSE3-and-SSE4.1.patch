From 99f24401e9b5444679a413bedbf8a0e63d9e56ce Mon Sep 17 00:00:00 2001
From: WuZhen <wuzhen@jidemail.com>
Date: Thu, 10 Mar 2016 17:39:15 +0800
Subject: [PATCH 060/106] x86: instruction set emulation for SSSE3 and SSE4.1

This commit implements the full set of SSSE3 and SSE4.1
instruction set and popcnt/movbe.

Known limitation:
1. SSSE3 instructions over MMX registers are not implemented
2. REX prefix support is not complete

Change-Id: I9b2d927b690b27460b9a944971587fe5cb7de8e1
---
 arch/x86/kernel/Makefile            |   3 +
 arch/x86/kernel/SSEPlus_REF.h       | 136 +++++
 arch/x86/kernel/SSEPlus_float_REF.c | 544 ++++++++++++++++++++
 arch/x86/kernel/SSEPlus_sse4_REF.c  | 659 ++++++++++++++++++++++++
 arch/x86/kernel/SSEPlus_ssse3_REF.c | 319 ++++++++++++
 arch/x86/kernel/SSEPlus_utils.c     | 230 +++++++++
 arch/x86/kernel/traps.c             | 745 +++++++++++++++++++++-------
 7 files changed, 2452 insertions(+), 184 deletions(-)
 create mode 100644 arch/x86/kernel/SSEPlus_REF.h
 create mode 100644 arch/x86/kernel/SSEPlus_float_REF.c
 create mode 100644 arch/x86/kernel/SSEPlus_sse4_REF.c
 create mode 100644 arch/x86/kernel/SSEPlus_ssse3_REF.c
 create mode 100644 arch/x86/kernel/SSEPlus_utils.c

diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index 5eeb808eb024..a912670840c6 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -51,6 +51,8 @@ CFLAGS_head$(BITS).o	+= -fno-stack-protector
 
 CFLAGS_irq.o := -I $(srctree)/$(src)/../include/asm/trace
 
+CFLAGS_REMOVE_SSEPlus_float_REF.o += -mno-sse -mno-mmx -mno-sse2 -msoft-float
+
 obj-y			:= process_$(BITS).o signal.o
 obj-$(CONFIG_COMPAT)	+= signal_compat.o
 obj-y			+= traps.o idt.o irq.o irq_$(BITS).o dumpstack_$(BITS).o
@@ -73,6 +75,7 @@ obj-y			+= pci-iommu_table.o
 obj-y			+= resource.o
 obj-y			+= irqflags.o
 obj-y			+= static_call.o
+obj-y			+= SSEPlus_float_REF.o SSEPlus_ssse3_REF.o SSEPlus_sse4_REF.o SSEPlus_utils.o
 
 obj-y				+= process.o
 obj-y				+= fpu/
diff --git a/arch/x86/kernel/SSEPlus_REF.h b/arch/x86/kernel/SSEPlus_REF.h
new file mode 100644
index 000000000000..4a781c924b4c
--- /dev/null
+++ b/arch/x86/kernel/SSEPlus_REF.h
@@ -0,0 +1,136 @@
+#ifndef __SSEPLUS_NUMBER_REF_H__
+#define __SSEPLUS_NUMBER_REF_H__
+
+
+
+typedef float              ssp_f32;
+typedef double             ssp_f64;
+
+typedef union {
+	u64 u64[2];
+	s64 s64[2];
+	ssp_f64 f64[2];
+	u32 u32[4];
+	s32 s32[4];
+	ssp_f32 f32[4];
+	u16 u16[8];
+	s16 s16[8];
+	u8 u8[16];
+	s8 s8[16];
+} ssp_m128 __aligned(16);
+
+struct pt_regs;
+
+// SSSE3
+
+void ssp_abs_epi8(ssp_m128 *A);
+void ssp_abs_epi16(ssp_m128 *A);
+void ssp_abs_epi32(ssp_m128 *A);
+ssp_m128 ssp_shuffle_epi8(ssp_m128 *A, ssp_m128 *MSK);
+void ssp_alignr_epi8(ssp_m128 *ret, ssp_m128 *a, ssp_m128 *b, const unsigned int ralign);
+ssp_m128 ssp_sign_epi8(ssp_m128* a, ssp_m128* b);
+ssp_m128 ssp_sign_epi16(ssp_m128* a, ssp_m128* b);
+ssp_m128 ssp_sign_epi32(ssp_m128* a, ssp_m128* b);
+ssp_m128 ssp_mulhrs_epi16( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_maddubs_epi16( ssp_m128* a,  ssp_m128* b);
+ssp_m128 ssp_hsub_epi16( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_hsub_epi32( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_hsubs_epi16( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_hadd_epi16( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_hadd_epi32( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_hadds_epi16( ssp_m128* a, ssp_m128* b );
+
+
+
+// SSE4.1
+
+ssp_m128 ssp_cvtepi8_epi16 ( ssp_m128* a);
+ssp_m128 ssp_cvtepi8_epi32 ( ssp_m128* a);
+ssp_m128 ssp_cvtepi8_epi64 ( ssp_m128* a);
+ssp_m128 ssp_cvtepi16_epi32 ( ssp_m128* a);
+ssp_m128 ssp_cvtepi16_epi64( ssp_m128* a);
+ssp_m128 ssp_cvtepi32_epi64 ( ssp_m128* a);
+ssp_m128 ssp_cvtepu8_epi16 ( ssp_m128* a);
+ssp_m128 ssp_cvtepu8_epi32 ( ssp_m128* a);
+ssp_m128 ssp_cvtepu8_epi64 ( ssp_m128* a);
+ssp_m128 ssp_cvtepu16_epi32 ( ssp_m128* a);
+ssp_m128 ssp_cvtepu16_epi64 ( ssp_m128* a);
+ssp_m128 ssp_cvtepu32_epi64 ( ssp_m128* a);
+ssp_m128 ssp_blend_epi16( ssp_m128* a, ssp_m128* b, const int mask );
+ssp_m128 ssp_blend_pd( ssp_m128* a, ssp_m128* b, const int mask );
+ssp_m128 ssp_blend_ps( ssp_m128* a, ssp_m128* b, const int mask );
+ssp_m128 ssp_blendv_epi8( ssp_m128* a, ssp_m128* b, ssp_m128* mask );
+ssp_m128 ssp_blendv_pd( ssp_m128* a, ssp_m128* b, ssp_m128* mask );
+ssp_m128 ssp_blendv_ps( ssp_m128* a, ssp_m128* b, ssp_m128* mask );
+ssp_m128 ssp_min_epi8( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_max_epi8( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_min_epu16 ( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_max_epu16 ( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_min_epi32( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_max_epi32( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_min_epu32 ( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_max_epu32 ( ssp_m128* a, ssp_m128* b );
+int ssp_testc_si128( ssp_m128* a, ssp_m128* b);
+int ssp_testz_si128( ssp_m128* a, ssp_m128* b);
+ssp_m128 ssp_cmpeq_epi64( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_packus_epi32( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_mpsadbw_epu8( ssp_m128* a,  ssp_m128* b,   const int msk  );
+ssp_m128 ssp_mul_epi32( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_mullo_epi32( ssp_m128* a, ssp_m128* b );
+ssp_m128 ssp_minpos_epu16( ssp_m128* shortValues );
+ssp_m128 ssp_insert_epi8( ssp_m128* a, int b, const int ndx );
+ssp_m128 ssp_insert_epi32( ssp_m128* a, int b, const int ndx );
+ssp_m128 ssp_insert_epi64( ssp_m128* a, s64 b, const int ndx );
+int ssp_extract_epi8( ssp_m128* a, const int ndx );
+int ssp_extract_epi32( ssp_m128* a, const int imm );
+s64 ssp_extract_epi64( ssp_m128* a, const int ndx );
+ssp_m128 ssp_stream_load_si128( ssp_m128 *p );
+
+// SSE4.1 floating point
+
+ssp_m128 ssp_round_pd( ssp_m128* val, int iRoundMode );
+ssp_m128 ssp_round_ps( ssp_m128* val, int iRoundMode );
+ssp_m128 ssp_round_sd( ssp_m128* dst, ssp_m128* val, int iRoundMode );
+ssp_m128 ssp_round_ss( ssp_m128* dst, ssp_m128* val, int iRoundMode );
+ssp_m128 ssp_dp_pd( ssp_m128* a, ssp_m128* b, const int mask );
+ssp_m128 ssp_dp_ps( ssp_m128* a, ssp_m128* b, const int mask );
+ssp_m128 ssp_insert_ps( ssp_m128* a, ssp_m128* b, const int sel );
+int ssp_extract_ps( ssp_m128* a, const int ndx );
+
+
+// popcnt
+
+unsigned short ssp_popcnt_16( unsigned short val );
+unsigned int ssp_popcnt_32( unsigned int val );
+u64 ssp_popcnt_64( u64 val );
+
+// utils
+
+ssp_m128 getXMMRegister(int index, int extended);
+void setXMMRegister(int index, int extended, ssp_m128* value);
+
+unsigned long* getRegisterPtr(int index, struct pt_regs* regs, int extended);
+void setRegister(int index, struct pt_regs* regs, int extended, unsigned long value);
+
+int decodeMemAddress(int opcode, struct pt_regs* regs, int rex, u8* extraBytes, unsigned long *memAddr);
+
+int getOp2MemValue(int opcode, struct pt_regs* regs, int rex, u8* extraBytes, unsigned long* value);
+int getOp2XMMValue(int opcode, struct pt_regs* regs, int rex, u8* extraBytes, ssp_m128* value);
+
+#define REX_B 0x41
+#define REX_X 0x42
+#define REX_R 0x44
+#define REX_W 0x48
+
+static inline
+int testREX(int rex, int mask)
+{
+	return (rex & mask) == mask;
+}
+
+
+/** @}
+ *  @}
+ */
+
+#endif // __SSEPLUS_NUMBER_REF_H__
diff --git a/arch/x86/kernel/SSEPlus_float_REF.c b/arch/x86/kernel/SSEPlus_float_REF.c
new file mode 100644
index 000000000000..b8f9c4d0e2f2
--- /dev/null
+++ b/arch/x86/kernel/SSEPlus_float_REF.c
@@ -0,0 +1,544 @@
+#include <linux/kernel.h>
+
+#include <asm/fpu/internal.h>
+
+#include "SSEPlus_REF.h"
+
+#define SSP_FROUND_TO_NEAREST_INT    0x00
+#define SSP_FROUND_TO_NEG_INF        0x01
+#define SSP_FROUND_TO_POS_INF        0x02
+#define SSP_FROUND_TO_ZERO           0x03
+#define SSP_FROUND_CUR_DIRECTION     0x04
+
+#define SSP_FROUND_RAISE_EXC         0x00
+#define SSP_FROUND_NO_EXC            0x08
+
+/** @addtogroup supplimental_REF
+ *  @{
+ *  @name Number Operations{
+ */
+
+static
+int ssp_number_isValidNumber_F32_REF( s32* val )//TODO: move into utility collection
+{
+	// Check for NAN, +infin, or -infin (exponent: 111 1111 1)
+	// Are the exponent bits all 1's?
+	if( (*val & 0x7F800000) == 0x7F800000 ) {
+		return 0;
+	}
+	return 1;
+}
+
+static
+int ssp_number_isValidNumber_F64_REF( s64* val )   //TODO: move into utility collection
+{
+	// Check for NAN, +infin, or -infin (exponent: 1111 1111)
+	// Are the exponent bits all 1's?
+	if( (*val & 0x7FF0000000000000ll) == 0x7FF0000000000000ll ) {
+		return 0;
+	}
+	return 1;
+}
+
+static
+ssp_f32 ssp_number_changeSNanToQNaN_F32_REF( s32* val )//TODO: move into utility collection
+{
+	ssp_f32* retVal = (ssp_f32*)val;
+	// Check if the value is already a QNaN
+	if( (*val & 0x00400000) != 0x00400000 ) {
+		// Check if the value is + or - infinitie
+		if( (*val | 0x7F800000) != 0x7F800000 ) {
+			// Convert SNan To QNaN
+			*retVal = (ssp_f32)( *val | 0x00400000 );
+		}
+	}
+	return *retVal;
+}
+
+static
+ssp_f64 ssp_number_changeSNanToQNaN_F64_REF( s64* val )//TODO: move into utility collection
+{
+	ssp_f64* retVal = (ssp_f64*)val;
+	// Check if the value is already a QNaN
+	if( (*val & 0x0008000000000000ll) != 0x0008000000000000ll ) {
+		// Check if the value is + or - infinitie
+		if( (*val | 0x7FF0000000000000ll) != 0x7FF0000000000000ll ) {
+			// Convert SNan To QNaN
+			*retVal = (ssp_f64)( *val | 0x0008000000000000ll );
+		}
+	}
+	return *retVal;
+}
+
+
+#define __HI(x) *(1+(int*)&x)
+#define __LO(x) *(int*)&x
+
+static const ssp_f64 huge = 1.0e300;
+
+static
+ssp_f64 ssp_ceil(ssp_f64 x)
+{
+	int i0,i1,j0;
+	unsigned i,j;
+	i0 =  __HI(x);
+	i1 =  __LO(x);
+	j0 = ((i0>>20)&0x7ff)-0x3ff;
+	if(j0<20) {
+		if(j0<0) {	/* raise inexact if x != 0 */
+			if(huge+x>0.0) {/* return 0*sign(x) if |x|<1 */
+				if(i0<0) {
+					i0=0x80000000;
+					i1=0;
+				} else if((i0|i1)!=0) {
+					i0=0x3ff00000;
+					i1=0;
+				}
+			}
+		} else {
+			i = (0x000fffff)>>j0;
+			if(((i0&i)|i1)==0)
+				return x; /* x is integral */
+			if(huge+x>0.0) {	/* raise inexact flag */
+				if(i0>0)
+					i0 += (0x00100000)>>j0;
+				i0 &= (~i); i1=0;
+			}
+		}
+	} else if (j0>51) {
+		if(j0==0x400)
+			return x+x;	/* inf or NaN */
+		else
+			return x;		/* x is integral */
+	} else {
+		i = ((unsigned)(0xffffffff))>>(j0-20);
+		if((i1&i)==0)
+			return x;	/* x is integral */
+		if(huge+x>0.0) {		/* raise inexact flag */
+			if(i0>0) {
+				if(j0==20)
+					i0+=1;
+				else {
+					j = i1 + (1<<(52-j0));
+					if(j<i1)
+						i0+=1;	/* got a carry */
+					i1 = j;
+				}
+			}
+			i1 &= (~i);
+		}
+	}
+	__HI(x) = i0;
+	__LO(x) = i1;
+	return x;
+}
+
+static
+ssp_f64 ssp_floor(ssp_f64 x)
+{
+	int i0,i1,j0;
+	unsigned i,j;
+	i0 =  __HI(x);
+	i1 =  __LO(x);
+	j0 = ((i0>>20)&0x7ff)-0x3ff;
+	if(j0<20) {
+		if(j0<0) {	/* raise inexact if x != 0 */
+			if(huge+x>0.0) {/* return 0*sign(x) if |x|<1 */
+				if(i0>=0)
+					i0=i1=0;
+				else if(((i0&0x7fffffff)|i1)!=0) {
+					i0=0xbff00000;
+					i1=0;
+				}
+			}
+		} else {
+			i = (0x000fffff)>>j0;
+			if(((i0&i)|i1)==0)
+				return x; /* x is integral */
+			if(huge+x>0.0) {	/* raise inexact flag */
+				if(i0<0)
+					i0 += (0x00100000)>>j0;
+				i0 &= (~i); i1=0;
+			}
+		}
+	} else if (j0>51) {
+		if(j0==0x400)
+			return x+x;	/* inf or NaN */
+		else
+			return x;		/* x is integral */
+	} else {
+		i = ((unsigned)(0xffffffff))>>(j0-20);
+		if((i1&i)==0)
+			return x;	/* x is integral */
+		if(huge+x>0.0) {		/* raise inexact flag */
+			if(i0<0) {
+				if(j0==20)
+					i0+=1;
+				else {
+					j = i1+(1<<(52-j0));
+					if(j<i1)
+						i0 +=1 ;	/* got a carry */
+					i1=j;
+				}
+			}
+			i1 &= (~i);
+		}
+	}
+	__HI(x) = i0;
+	__LO(x) = i1;
+	return x;
+}
+
+ssp_m128 ssp_round_pd( ssp_m128* val, int iRoundMode )
+{
+	s64 *valPtr;
+	ssp_m128 Val;
+	Val = *val;
+
+	kernel_fpu_begin();
+
+	switch( iRoundMode & 0x3 )
+	{
+	case SSP_FROUND_CUR_DIRECTION:
+		break;
+	case SSP_FROUND_TO_ZERO:
+		valPtr = (s64*)(&Val.f64[0]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Val.f64[0] = (ssp_f64)( (s64)Val.f64[0] );
+
+		valPtr = (s64*)(&Val.f64[1]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Val.f64[1] = (ssp_f64)( (s64)Val.f64[1] );
+		break;
+	case SSP_FROUND_TO_POS_INF:
+		valPtr = (s64*)(&Val.f64[0]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Val.f64[0] = ssp_ceil( Val.f64[0] );
+
+		valPtr = (s64*)(&Val.f64[1]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Val.f64[1] = ssp_ceil( Val.f64[1] );
+		break;
+	case SSP_FROUND_TO_NEG_INF:
+		valPtr = (s64*)(&Val.f64[0]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Val.f64[0] = ssp_floor( Val.f64[0] );
+
+		valPtr = (s64*)(&Val.f64[1]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Val.f64[1] = ssp_floor( Val.f64[1] );
+		break;
+	default: // SSP_FROUND_TO_NEAREST_INT
+		valPtr = (s64*)(&Val.f64[0]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Val.f64[0] = (ssp_f64)( (Val.f64[0]>0) ? (s64)(Val.f64[0]+0.5) : (s64)(Val.f64[0]-0.5) );
+		else
+			Val.f64[0] = ssp_number_changeSNanToQNaN_F64_REF( valPtr );
+
+		valPtr = (s64*)(&Val.f64[1]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Val.f64[1] = (ssp_f64)( (Val.f64[1]>0) ? (s64)(Val.f64[1]+0.5) : (s64)(Val.f64[1]-0.5) );
+		else
+			Val.f64[1] = ssp_number_changeSNanToQNaN_F64_REF( valPtr );
+	}
+
+	kernel_fpu_end();
+
+	return Val;
+}
+
+ssp_m128 ssp_round_ps( ssp_m128* val, int iRoundMode )
+{
+	s32 *valPtr;
+	ssp_m128 Val;
+	Val = *val;
+
+	kernel_fpu_begin();
+
+	switch( iRoundMode & 0x3 )
+	{
+	case SSP_FROUND_CUR_DIRECTION:
+		break;
+	case SSP_FROUND_TO_ZERO:
+		valPtr = (s32*)(&Val.f32[0]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) ) {
+			if( Val.f32[0] >= 0 )
+				Val.f32[0] = (ssp_f32)( (s32)Val.f32[0] );
+			else {
+				Val.f32[0] = (ssp_f32)( (s32)Val.f32[0] );
+				//Val.s32[0] = Val.s32[0] | 0x80000000;
+			}
+		}
+
+		valPtr = (s32*)(&Val.f32[1]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) ) {
+			if( Val.f32[1] >= 0 )
+				Val.f32[1] = (ssp_f32)( (s32)Val.f32[1] );
+			else {
+				Val.f32[1] = (ssp_f32)( (s32)Val.f32[1] );
+				//Val.s32[1] = Val.s32[1] | 0x80000000;
+			}
+		}
+
+		valPtr = (s32*)(&Val.f32[2]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) ) {
+			if( Val.f32[2] >= 0 )
+				Val.f32[2] = (ssp_f32)( (s32)Val.f32[2] );
+			else {
+				Val.f32[2] = (ssp_f32)( (s32)Val.f32[2] );
+				//Val.s32[2] = Val.s32[2] | 0x80000000;
+			}
+		}
+
+		valPtr = (s32*)(&Val.f32[3]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) ) {
+			if( Val.f32[3] >= 0 )
+				Val.f32[3] = (ssp_f32)( (s32)Val.f32[3] );
+			else {
+				Val.f32[3] = (ssp_f32)( (s32)Val.f32[3] );
+				//Val.s32[3] = Val.s32[3] | 0x80000000;
+			}
+		}
+		break;
+	case SSP_FROUND_TO_POS_INF:
+		valPtr = (s32*)(&Val.f32[0]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[0] = (ssp_f32)ssp_ceil( Val.f32[0] );
+
+		valPtr = (s32*)(&Val.f32[1]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[1] = (ssp_f32)ssp_ceil( Val.f32[1] );
+
+		valPtr = (s32*)(&Val.f32[2]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[2] = (ssp_f32)ssp_ceil( Val.f32[2] );
+
+		valPtr = (s32*)(&Val.f32[3]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[3] = (ssp_f32)ssp_ceil( Val.f32[3] );
+		break;
+	case SSP_FROUND_TO_NEG_INF:
+		valPtr = (s32*)(&Val.f32[0]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[0] = (ssp_f32)ssp_floor( Val.f32[0] );
+
+		valPtr = (s32*)(&Val.f32[1]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[1] = (ssp_f32)ssp_floor( Val.f32[1] );
+
+		valPtr = (s32*)(&Val.f32[2]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[2] = (ssp_f32)ssp_floor( Val.f32[2] );
+
+		valPtr = (s32*)(&Val.f32[3]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[3] = (ssp_f32)ssp_floor( Val.f32[3] );
+		break;
+	default: // SSP_FROUND_TO_NEAREST_INT
+		valPtr = (s32*)(&Val.f32[0]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[0] = (ssp_f32)( (Val.f32[0]>0) ? (s32)(Val.f32[0]+0.5) : (s32)(Val.f32[0]-0.5) );
+		else
+			Val.f32[0] = ssp_number_changeSNanToQNaN_F32_REF( valPtr );
+
+		valPtr = (s32*)(&Val.f32[1]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[1] = (ssp_f32)( (Val.f32[1]>0) ? (s32)(Val.f32[1]+0.5) : (s32)(Val.f32[1]-0.5) );
+		else
+			Val.f32[1] = ssp_number_changeSNanToQNaN_F32_REF( valPtr );
+
+		valPtr = (s32*)(&Val.f32[2]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[2] = (ssp_f32)( (Val.f32[2]>0) ? (s32)(Val.f32[2]+0.5) : (s32)(Val.f32[2]-0.5) );
+		else
+			Val.f32[2] = ssp_number_changeSNanToQNaN_F32_REF( valPtr );
+
+		valPtr = (s32*)(&Val.f32[3]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Val.f32[3] = (ssp_f32)( (Val.f32[3]>0) ? (s32)(Val.f32[3]+0.5) : (s32)(Val.f32[3]-0.5) );
+		else
+			Val.f32[3] = ssp_number_changeSNanToQNaN_F32_REF( valPtr );
+	}
+
+	if( -0.0f == Val.f32[0] ) Val.f32[0]=+0.0f;
+	if( -0.0f == Val.f32[1] ) Val.f32[1]=+0.0f;
+	if( -0.0f == Val.f32[2] ) Val.f32[2]=+0.0f;
+	if( -0.0f == Val.f32[3] ) Val.f32[3]=+0.0f;
+
+	kernel_fpu_end();
+
+	return Val;
+}
+
+ssp_m128 ssp_round_sd( ssp_m128* dst, ssp_m128* val, int iRoundMode )
+{
+	s64 *valPtr;
+	ssp_m128 Dst, Val;
+	Dst = *dst;
+	Val = *val;
+
+	kernel_fpu_begin();
+
+	switch( iRoundMode & 0x3 )
+	{
+	case SSP_FROUND_CUR_DIRECTION:
+		break;
+	case SSP_FROUND_TO_ZERO:
+		valPtr = (s64*)(&Val.f64[0]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Dst.f64[0] = (ssp_f64)( (s64)Val.f64[0] );
+		break;
+	case SSP_FROUND_TO_POS_INF:
+		valPtr = (s64*)(&Val.f64[0]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Dst.f64[0] = ssp_ceil( Val.f64[0] );
+		break;
+	case SSP_FROUND_TO_NEG_INF:
+		valPtr = (s64*)(&Val.f64[0]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Dst.f64[0] = ssp_floor( Val.f64[0] );
+		break;
+	default: // SSP_FROUND_TO_NEAREST_INT
+		valPtr = (s64*)(&Val.f64[0]);
+		if( ssp_number_isValidNumber_F64_REF( valPtr ) )
+			Dst.f64[0] = (ssp_f64)( (Val.f64[0]>0) ? (s64)(Val.f64[0]+0.5) : (s64)(Val.f64[0]-0.5) );
+		else
+			Dst.f64[0] = ssp_number_changeSNanToQNaN_F64_REF( valPtr );
+	}
+
+	kernel_fpu_end();
+
+	return Dst;
+}
+
+ssp_m128 ssp_round_ss( ssp_m128* dst, ssp_m128* val, int iRoundMode )        //_mm_round_ss
+{
+	s32 *valPtr;
+	ssp_m128 Dst, Val;
+	Dst = *dst;
+	Val = *val;
+
+	kernel_fpu_begin();
+
+	switch( iRoundMode & 0x3 )
+	{
+	case SSP_FROUND_CUR_DIRECTION:
+		break;
+	case SSP_FROUND_TO_ZERO:
+		valPtr = (s32*)(&Val.f32[0]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) ) {
+			Dst.f32[0] = (ssp_f32)( (s32)Val.f32[0] );
+			if( Val.f32[0] <= -0 )
+				Dst.s32[0] = Dst.s32[0] | 0x80000000;
+		}
+		break;
+	case SSP_FROUND_TO_POS_INF:
+		valPtr = (s32*)(&Val.f32[0]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Dst.f32[0] = (ssp_f32)ssp_ceil( Val.f32[0] );
+		break;
+	case SSP_FROUND_TO_NEG_INF:
+		valPtr = (s32*)(&Val.f32[0]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Dst.f32[0] = (ssp_f32)ssp_floor( Val.f32[0] );
+		break;
+	default: // SSP_FROUND_TO_NEAREST_INT
+		valPtr = (s32*)(&Val.f32[0]);
+		if( ssp_number_isValidNumber_F32_REF( valPtr ) )
+			Dst.f32[0] = (ssp_f32)( (Val.f32[0]>0) ? (s32)(Val.f32[0]+0.5) : (s32)(Val.f32[0]-0.5) );
+		else
+			Dst.f32[0] = ssp_number_changeSNanToQNaN_F32_REF( valPtr );
+	}
+
+	kernel_fpu_end();
+
+	return Dst;
+}
+
+ssp_m128 ssp_dp_pd( ssp_m128* a, ssp_m128* b, const int mask )
+{
+	ssp_f64 tmp[3];
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	kernel_fpu_begin();
+
+	tmp[0] = (mask & 0x10) ? (A.f64[0] * B.f64[0]) : 0.0;
+	tmp[1] = (mask & 0x20) ? (A.f64[1] * B.f64[1]) : 0.0;
+
+	tmp[2] = tmp[0] + tmp[1];
+
+	A.f64[0] = (mask & 0x1) ? tmp[2] : 0.0;
+	A.f64[1] = (mask & 0x2) ? tmp[2] : 0.0;
+
+	kernel_fpu_end();
+
+	return A;
+}
+
+ssp_m128 ssp_dp_ps( ssp_m128* a, ssp_m128* b, const int mask )
+{
+	ssp_f32 tmp[5];
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	kernel_fpu_begin();
+
+	tmp[0] = (mask & 0x10) ? (A.f32[0] * B.f32[0]) : 0.0f;
+	tmp[1] = (mask & 0x20) ? (A.f32[1] * B.f32[1]) : 0.0f;
+	tmp[2] = (mask & 0x40) ? (A.f32[2] * B.f32[2]) : 0.0f;
+	tmp[3] = (mask & 0x80) ? (A.f32[3] * B.f32[3]) : 0.0f;
+
+	tmp[4] = tmp[0] + tmp[1] + tmp[2] + tmp[3];
+
+	A.f32[0] = (mask & 0x1) ? tmp[4] : 0.0f;
+	A.f32[1] = (mask & 0x2) ? tmp[4] : 0.0f;
+	A.f32[2] = (mask & 0x4) ? tmp[4] : 0.0f;
+	A.f32[3] = (mask & 0x8) ? tmp[4] : 0.0f;
+
+	kernel_fpu_end();
+
+	return A;
+}
+
+ssp_m128 ssp_insert_ps( ssp_m128* a, ssp_m128* b, const int sel )          // Verify behavior on Intel Hardware
+{
+	ssp_f32 tmp;
+	int count_d,zmask;
+
+	kernel_fpu_begin();
+
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	tmp     = B.f32[(sel & 0xC0)>>6];   // 0xC0 = sel[7:6]
+	count_d = (sel & 0x30)>>4;          // 0x30 = sel[5:4]
+	zmask   = sel & 0x0F;               // 0x0F = sel[3:0]
+
+	A.f32[count_d] = tmp;
+
+	A.f32[0] = (zmask & 0x1) ? 0 : A.f32[0];
+	A.f32[1] = (zmask & 0x2) ? 0 : A.f32[1];
+	A.f32[2] = (zmask & 0x4) ? 0 : A.f32[2];
+	A.f32[3] = (zmask & 0x8) ? 0 : A.f32[3];
+
+	kernel_fpu_end();
+
+	return A;
+}
+
+int ssp_extract_ps( ssp_m128* a, const int ndx )
+{
+	ssp_m128 A;
+
+	kernel_fpu_begin();
+
+	A.f32[ndx&0x3] = a->f32[ndx&0x3];
+
+	kernel_fpu_end();
+
+	return A.s32[ndx&0x3];
+}
diff --git a/arch/x86/kernel/SSEPlus_sse4_REF.c b/arch/x86/kernel/SSEPlus_sse4_REF.c
new file mode 100644
index 000000000000..93e42fe8bc0b
--- /dev/null
+++ b/arch/x86/kernel/SSEPlus_sse4_REF.c
@@ -0,0 +1,659 @@
+#include <linux/kernel.h>
+
+#include "SSEPlus_REF.h"
+
+ssp_m128 ssp_cvtepi8_epi16 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s16[7] = A.s8[7];
+	A.s16[6] = A.s8[6];
+	A.s16[5] = A.s8[5];
+	A.s16[4] = A.s8[4];
+	A.s16[3] = A.s8[3];
+	A.s16[2] = A.s8[2];
+	A.s16[1] = A.s8[1];
+	A.s16[0] = A.s8[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepi8_epi32 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s32[3] = A.s8[3];
+	A.s32[2] = A.s8[2];
+	A.s32[1] = A.s8[1];
+	A.s32[0] = A.s8[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepi8_epi64 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s64[1] = A.s8[1];
+	A.s64[0] = A.s8[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepi16_epi32 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s32[3] = A.s16[3];
+	A.s32[2] = A.s16[2];
+	A.s32[1] = A.s16[1];
+	A.s32[0] = A.s16[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepi16_epi64( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s64[1] = A.s16[1];
+	A.s64[0] = A.s16[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepi32_epi64 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s64[1] = A.s32[1];
+	A.s64[0] = A.s32[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepu8_epi16 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s16[7] = A.u8[7];
+	A.s16[6] = A.u8[6];
+	A.s16[5] = A.u8[5];
+	A.s16[4] = A.u8[4];
+	A.s16[3] = A.u8[3];
+	A.s16[2] = A.u8[2];
+	A.s16[1] = A.u8[1];
+	A.s16[0] = A.u8[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepu8_epi32 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s32[3] = A.u8[3];
+	A.s32[2] = A.u8[2];
+	A.s32[1] = A.u8[1];
+	A.s32[0] = A.u8[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepu8_epi64 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s64[1] = A.u8[1];
+	A.s64[0] = A.u8[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepu16_epi32 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s32[3] = A.u16[3];
+	A.s32[2] = A.u16[2];
+	A.s32[1] = A.u16[1];
+	A.s32[0] = A.u16[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepu16_epi64 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s64[1] = A.u16[1];
+	A.s64[0] = A.u16[0];
+	return A;
+}
+
+ssp_m128 ssp_cvtepu32_epi64 ( ssp_m128* a)
+{
+	ssp_m128 A = *a;
+
+	A.s64[1] = A.u32[1];
+	A.s64[0] = A.u32[0];
+	return A;
+}
+
+#define SSP_SET_MIN( sd, s) sd=(sd<s)?sd:s;
+#define SSP_SET_MAX( sd, s) sd=(sd>s)?sd:s;
+
+ssp_m128 ssp_blend_epi16( ssp_m128* a, ssp_m128* b, const int mask )
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.s16[0] = (mask & 0x01) ? B.s16[0] : A.s16[0];
+	A.s16[1] = (mask & 0x02) ? B.s16[1] : A.s16[1];
+	A.s16[2] = (mask & 0x04) ? B.s16[2] : A.s16[2];
+	A.s16[3] = (mask & 0x08) ? B.s16[3] : A.s16[3];
+	A.s16[4] = (mask & 0x10) ? B.s16[4] : A.s16[4];
+	A.s16[5] = (mask & 0x20) ? B.s16[5] : A.s16[5];
+	A.s16[6] = (mask & 0x40) ? B.s16[6] : A.s16[6];
+	A.s16[7] = (mask & 0x80) ? B.s16[7] : A.s16[7];
+	return A;
+}
+
+ssp_m128 ssp_blend_pd( ssp_m128* a, ssp_m128* b, const int mask )
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.f64[0] = (mask & 0x1) ? B.f64[0] : A.f64[0];
+	A.f64[1] = (mask & 0x2) ? B.f64[1] : A.f64[1];
+
+	return A;
+}
+
+ssp_m128 ssp_blend_ps( ssp_m128* a, ssp_m128* b, const int mask )
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.f32[0] = (mask & 0x1) ? B.f32[0] : A.f32[0];
+	A.f32[1] = (mask & 0x2) ? B.f32[1] : A.f32[1];
+	A.f32[2] = (mask & 0x4) ? B.f32[2] : A.f32[2];
+	A.f32[3] = (mask & 0x8) ? B.f32[3] : A.f32[3];
+
+	return A;
+}
+
+ssp_m128 ssp_blendv_epi8( ssp_m128* a, ssp_m128* b, ssp_m128* mask )
+{
+	ssp_m128 A, B, Mask;
+	A = *a;
+	B = *b;
+	Mask = *mask;
+
+	A.s8[0]  = (Mask.s8[0]  & 0x80) ? B.s8[0]  : A.s8[0];
+	A.s8[1]  = (Mask.s8[1]  & 0x80) ? B.s8[1]  : A.s8[1];
+	A.s8[2]  = (Mask.s8[2]  & 0x80) ? B.s8[2]  : A.s8[2];
+	A.s8[3]  = (Mask.s8[3]  & 0x80) ? B.s8[3]  : A.s8[3];
+	A.s8[4]  = (Mask.s8[4]  & 0x80) ? B.s8[4]  : A.s8[4];
+	A.s8[5]  = (Mask.s8[5]  & 0x80) ? B.s8[5]  : A.s8[5];
+	A.s8[6]  = (Mask.s8[6]  & 0x80) ? B.s8[6]  : A.s8[6];
+	A.s8[7]  = (Mask.s8[7]  & 0x80) ? B.s8[7]  : A.s8[7];
+	A.s8[8]  = (Mask.s8[8]  & 0x80) ? B.s8[8]  : A.s8[8];
+	A.s8[9]  = (Mask.s8[9]  & 0x80) ? B.s8[9]  : A.s8[9];
+	A.s8[10] = (Mask.s8[10] & 0x80) ? B.s8[10] : A.s8[10];
+	A.s8[11] = (Mask.s8[11] & 0x80) ? B.s8[11] : A.s8[11];
+	A.s8[12] = (Mask.s8[12] & 0x80) ? B.s8[12] : A.s8[12];
+	A.s8[13] = (Mask.s8[13] & 0x80) ? B.s8[13] : A.s8[13];
+	A.s8[14] = (Mask.s8[14] & 0x80) ? B.s8[14] : A.s8[14];
+	A.s8[15] = (Mask.s8[15] & 0x80) ? B.s8[15] : A.s8[15];
+	return A;
+}
+
+ssp_m128 ssp_blendv_pd( ssp_m128* a, ssp_m128* b, ssp_m128* mask )
+{
+	ssp_m128 A, B, Mask;
+	A = *a;
+	B = *b;
+	Mask = *mask;
+
+	A.f64[0] = (Mask.u64[0] & 0x8000000000000000ull) ? B.f64[0] : A.f64[0];
+	A.f64[1] = (Mask.u64[1] & 0x8000000000000000ull) ? B.f64[1] : A.f64[1];
+
+	return A;
+}
+
+ssp_m128 ssp_blendv_ps( ssp_m128* a, ssp_m128* b, ssp_m128* mask )
+{
+	ssp_m128 A, B, Mask;
+	A = *a;
+	B = *b;
+	Mask = *mask;
+
+	A.f32[0] = (Mask.u32[0] & 0x80000000) ? B.f32[0] : A.f32[0];
+	A.f32[1] = (Mask.u32[1] & 0x80000000) ? B.f32[1] : A.f32[1];
+	A.f32[2] = (Mask.u32[2] & 0x80000000) ? B.f32[2] : A.f32[2];
+	A.f32[3] = (Mask.u32[3] & 0x80000000) ? B.f32[3] : A.f32[3];
+
+	return A;
+}
+
+ssp_m128 ssp_min_epi8( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	SSP_SET_MIN( A.s8[ 0], B.s8[ 0] );
+	SSP_SET_MIN( A.s8[ 1], B.s8[ 1] );
+	SSP_SET_MIN( A.s8[ 2], B.s8[ 2] );
+	SSP_SET_MIN( A.s8[ 3], B.s8[ 3] );
+	SSP_SET_MIN( A.s8[ 4], B.s8[ 4] );
+	SSP_SET_MIN( A.s8[ 5], B.s8[ 5] );
+	SSP_SET_MIN( A.s8[ 6], B.s8[ 6] );
+	SSP_SET_MIN( A.s8[ 7], B.s8[ 7] );
+	SSP_SET_MIN( A.s8[ 8], B.s8[ 8] );
+	SSP_SET_MIN( A.s8[ 9], B.s8[ 9] );
+	SSP_SET_MIN( A.s8[10], B.s8[10] );
+	SSP_SET_MIN( A.s8[11], B.s8[11] );
+	SSP_SET_MIN( A.s8[12], B.s8[12] );
+	SSP_SET_MIN( A.s8[13], B.s8[13] );
+	SSP_SET_MIN( A.s8[14], B.s8[14] );
+	SSP_SET_MIN( A.s8[15], B.s8[15] );
+	return A;
+}
+
+ssp_m128 ssp_max_epi8( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	SSP_SET_MAX( A.s8[ 0], B.s8[ 0] );
+	SSP_SET_MAX( A.s8[ 1], B.s8[ 1] );
+	SSP_SET_MAX( A.s8[ 2], B.s8[ 2] );
+	SSP_SET_MAX( A.s8[ 3], B.s8[ 3] );
+	SSP_SET_MAX( A.s8[ 4], B.s8[ 4] );
+	SSP_SET_MAX( A.s8[ 5], B.s8[ 5] );
+	SSP_SET_MAX( A.s8[ 6], B.s8[ 6] );
+	SSP_SET_MAX( A.s8[ 7], B.s8[ 7] );
+	SSP_SET_MAX( A.s8[ 8], B.s8[ 8] );
+	SSP_SET_MAX( A.s8[ 9], B.s8[ 9] );
+	SSP_SET_MAX( A.s8[10], B.s8[10] );
+	SSP_SET_MAX( A.s8[11], B.s8[11] );
+	SSP_SET_MAX( A.s8[12], B.s8[12] );
+	SSP_SET_MAX( A.s8[13], B.s8[13] );
+	SSP_SET_MAX( A.s8[14], B.s8[14] );
+	SSP_SET_MAX( A.s8[15], B.s8[15] );
+	return A;
+}
+
+ssp_m128 ssp_min_epu16 ( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	SSP_SET_MIN( A.u16[ 0], B.u16[ 0] );
+	SSP_SET_MIN( A.u16[ 1], B.u16[ 1] );
+	SSP_SET_MIN( A.u16[ 2], B.u16[ 2] );
+	SSP_SET_MIN( A.u16[ 3], B.u16[ 3] );
+	SSP_SET_MIN( A.u16[ 4], B.u16[ 4] );
+	SSP_SET_MIN( A.u16[ 5], B.u16[ 5] );
+	SSP_SET_MIN( A.u16[ 6], B.u16[ 6] );
+	SSP_SET_MIN( A.u16[ 7], B.u16[ 7] );
+	return A;
+}
+
+ssp_m128 ssp_max_epu16 ( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	SSP_SET_MAX( A.u16[ 0], B.u16[ 0] );
+	SSP_SET_MAX( A.u16[ 1], B.u16[ 1] );
+	SSP_SET_MAX( A.u16[ 2], B.u16[ 2] );
+	SSP_SET_MAX( A.u16[ 3], B.u16[ 3] );
+	SSP_SET_MAX( A.u16[ 4], B.u16[ 4] );
+	SSP_SET_MAX( A.u16[ 5], B.u16[ 5] );
+	SSP_SET_MAX( A.u16[ 6], B.u16[ 6] );
+	SSP_SET_MAX( A.u16[ 7], B.u16[ 7] );
+	return A;
+}
+
+ssp_m128 ssp_min_epi32( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	SSP_SET_MIN( A.s32[ 0], B.s32[ 0] );
+	SSP_SET_MIN( A.s32[ 1], B.s32[ 1] );
+	SSP_SET_MIN( A.s32[ 2], B.s32[ 2] );
+	SSP_SET_MIN( A.s32[ 3], B.s32[ 3] );
+	return A;
+}
+
+ssp_m128 ssp_max_epi32( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	SSP_SET_MAX( A.s32[ 0], B.s32[ 0] );
+	SSP_SET_MAX( A.s32[ 1], B.s32[ 1] );
+	SSP_SET_MAX( A.s32[ 2], B.s32[ 2] );
+	SSP_SET_MAX( A.s32[ 3], B.s32[ 3] );
+	return A;
+}
+
+ssp_m128 ssp_min_epu32 ( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	SSP_SET_MIN( A.u32[ 0], B.u32[ 0] );
+	SSP_SET_MIN( A.u32[ 1], B.u32[ 1] );
+	SSP_SET_MIN( A.u32[ 2], B.u32[ 2] );
+	SSP_SET_MIN( A.u32[ 3], B.u32[ 3] );
+	return A;
+}
+
+ssp_m128 ssp_max_epu32 ( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	SSP_SET_MAX( A.u32[ 0], B.u32[ 0] );
+	SSP_SET_MAX( A.u32[ 1], B.u32[ 1] );
+	SSP_SET_MAX( A.u32[ 2], B.u32[ 2] );
+	SSP_SET_MAX( A.u32[ 3], B.u32[ 3] );
+	return A;
+}
+
+#undef SSP_SET_MIN
+#undef SSP_SET_MAX
+
+int ssp_testc_si128( ssp_m128* a, ssp_m128* b)
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	return ( (A.s64[0] & B.s64[0]) == A.s64[0] ) &&
+		   ( (A.s64[1] & B.s64[1]) == A.s64[1] ) ;
+}
+
+int ssp_testz_si128( ssp_m128* a, ssp_m128* b)
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	return ( (A.s64[0] & B.s64[0]) == 0 ) &&
+		   ( (A.s64[1] & B.s64[1]) == 0 ) ;
+}
+
+ssp_m128 ssp_cmpeq_epi64( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	if( A.s64[0] == B.s64[0] )
+		A.s64[0] = 0xFFFFFFFFFFFFFFFFll;
+	else
+		A.s64[0] = 0x0ll;
+
+	if( A.s64[1] == B.s64[1] )
+		A.s64[1] = 0xFFFFFFFFFFFFFFFFll;
+	else
+		A.s64[1] = 0x0ll;
+	return A;
+}
+
+ssp_m128 ssp_packus_epi32( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	if( A.s32[0] < 0 )
+		A.u16[0] = 0;
+	else
+		if( A.s32[0] > 0xFFFF )
+			A.u16[0] = 0xFFFF;
+		else
+			A.s16[0] = (u16)A.s32[0];
+
+	if( A.s32[1] < 0 )
+		A.u16[1] = 0;
+	else
+		if( A.s32[1] > 0xFFFF )
+			A.u16[1] = 0xFFFF;
+		else
+			A.s16[1] = (u16)A.s32[1];
+
+	if( A.s32[2] < 0 )
+		A.u16[2] = 0;
+	else
+		if( A.s32[2] > 0xFFFF )
+			A.u16[2] = 0xFFFF;
+		else
+			A.s16[2] = (u16)A.s32[2];
+
+
+	if( A.s32[3] < 0 )
+		A.u16[3] = 0;
+	else
+		if( A.s32[3] > 0xFFFF )
+			A.u16[3] = 0xFFFF;
+		else
+			A.s16[3] = (u16)A.s32[3];
+
+	if( B.s32[0] < 0 )
+		A.u16[4] = 0;
+	else
+		if( B.s32[0] > 0xFFFF )
+			A.u16[4] = 0xFFFF;
+		else
+			A.s16[4] = (u16)B.s32[0];
+
+	if( B.s32[1] < 0 )
+		A.u16[5] = 0;
+	else
+		if( B.s32[1] > 0xFFFF )
+			A.u16[5] = 0xFFFF;
+		else
+			A.s16[5] = (u16)B.s32[1];
+
+	if( B.s32[2] < 0 )
+		A.u16[6] = 0;
+	else
+		if( B.s32[2] > 0xFFFF )
+			A.u16[6] = 0xFFFF;
+		else
+			A.s16[6] = (u16)B.s32[2];
+
+
+	if( B.s32[3] < 0 )
+		A.u16[7] = 0;
+	else
+		if( B.s32[3] > 0xFFFF )
+			A.u16[7] = 0xFFFF;
+		else
+			A.s16[7] = (u16)B.s32[3];
+
+	return A;
+}
+
+ssp_m128 ssp_mpsadbw_epu8( ssp_m128* a,  ssp_m128* b,   const int msk  )
+{
+	u8 Abyte[11], Bbyte[4], tmp[4];
+	u8 Boffset, Aoffset;
+	int i;
+
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	Boffset = (msk & 0x3) << 2; // *32/8,   for byte size count
+	Aoffset = (msk & 0x4);      // *32/8/4, for byte size count and shift msk to bit 2
+
+	for (i=0; i<11; i++)
+		Abyte[i] = A.u8[i+Aoffset];
+
+	Bbyte[0] = B.u8[Boffset  ];
+	Bbyte[1] = B.u8[Boffset+1];
+	Bbyte[2] = B.u8[Boffset+2];
+	Bbyte[3] = B.u8[Boffset+3];
+
+	for (i=0; i<8; i++) {
+		tmp[0] = (Abyte[i  ] > Bbyte[0]) ? (Abyte[i  ] - Bbyte[0]) :  (Bbyte[0] - Abyte[i  ]);        //abs diff
+		tmp[1] = (Abyte[i+1] > Bbyte[1]) ? (Abyte[i+1] - Bbyte[1]) :  (Bbyte[1] - Abyte[i+1]);
+		tmp[2] = (Abyte[i+2] > Bbyte[2]) ? (Abyte[i+2] - Bbyte[2]) :  (Bbyte[2] - Abyte[i+2]);
+		tmp[3] = (Abyte[i+3] > Bbyte[3]) ? (Abyte[i+3] - Bbyte[3]) :  (Bbyte[3] - Abyte[i+3]);
+
+		A.u16[i] = tmp[0] + tmp[1] + tmp[2] + tmp[3];
+	}
+
+	return A;
+}
+
+ssp_m128 ssp_mul_epi32( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	A.s64[0] = A.s32[0] * B.s32[0];
+	A.s64[1] = A.s32[2] * B.s32[2];
+	return A;
+}
+
+ssp_m128 ssp_mullo_epi32( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 t[2];
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	t[0].s64[0] = A.s32[0] * B.s32[0];
+	t[0].s64[1] = A.s32[1] * B.s32[1];
+	t[1].s64[0] = A.s32[2] * B.s32[2];
+	t[1].s64[1] = A.s32[3] * B.s32[3];
+
+	A.s32[0] = t[0].s32[0];
+	A.s32[1] = t[0].s32[2];
+	A.s32[2] = t[1].s32[0];
+	A.s32[3] = t[1].s32[2];
+	return A;
+}
+
+ssp_m128 ssp_minpos_epu16( ssp_m128* shortValues )
+{
+	ssp_m128 ShortValues;
+	ShortValues = *shortValues;
+
+	if( ShortValues.u16[1] < ShortValues.u16[0] ) {
+		ShortValues.u16[0] = ShortValues.u16[1];
+		ShortValues.u16[1] = 1;
+	} else
+		ShortValues.u16[1] = 0;
+
+
+#define FN( I )                                     \
+	if( ShortValues.u16[I] < ShortValues.u16[0] ) { \
+		ShortValues.u16[0] = ShortValues.u16[I];    \
+		ShortValues.u16[1] = I;                     \
+	}
+
+	FN( 2 );
+	FN( 3 );
+	FN( 4 );
+	FN( 5 );
+	FN( 6 );
+	FN( 7 );
+
+	ShortValues.u32[1] = 0;
+	ShortValues.u64[1] = 0;
+
+#undef FN
+
+	return ShortValues;
+}
+
+ssp_m128 ssp_insert_epi8( ssp_m128* a, int b, const int ndx )       // Verify behavior on Intel Hardware
+{
+	ssp_m128 A;
+	A = *a;
+
+	A.s8[ndx & 0xF] = (s8)b;
+	return A;
+}
+
+ssp_m128 ssp_insert_epi32( ssp_m128* a, int b, const int ndx )      // Verify behavior on Intel Hardware
+{
+	ssp_m128 A;
+	A = *a;
+
+	A.s32[ndx & 0x3] = b;
+	return A;
+}
+
+ssp_m128 ssp_insert_epi64( ssp_m128* a, s64 b, const int ndx )  // Verify behavior on Intel Hardware
+{
+	ssp_m128 A;
+	A = *a;
+
+	A.s64[ndx & 0x1] = b;
+	return A;
+}
+
+int ssp_extract_epi8( ssp_m128* a, const int ndx )
+{
+	ssp_m128 A;
+	A = *a;
+	return (int)A.u8[ndx&0xF];
+}
+
+int ssp_extract_epi32( ssp_m128* a, const int imm )
+{
+	ssp_m128 A;
+	A = *a;
+	return (int)A.u32[imm&0x3];
+}
+
+s64 ssp_extract_epi64( ssp_m128* a, const int ndx )
+{
+	ssp_m128 A;
+	A = *a;
+	return A.s64[ndx & 0x1];
+}
+
+ssp_m128 ssp_stream_load_si128( ssp_m128 *p )
+{
+	return *p;
+}
+
+unsigned short ssp_popcnt_16( unsigned short val )
+{
+	int i;
+	u16 cnt=0;
+	for( i=0; i<15, val; ++i, val = val>>1 )
+		cnt += val & 0x1;
+	return cnt;
+}
+
+unsigned int ssp_popcnt_32( unsigned int val )
+{
+	int i;
+	u32 cnt = 0;
+	for( i=0; i<31, val; ++i, val = val>>1 )
+		cnt += val & 0x1;
+	return cnt;
+}
+
+u64 ssp_popcnt_64( u64 val )
+{
+	int i;
+	u64 cnt = 0;
+	for( i=0; i<63, val; ++i, val = val>>1 )
+		cnt += val & 0x1;
+	return cnt;
+}
diff --git a/arch/x86/kernel/SSEPlus_ssse3_REF.c b/arch/x86/kernel/SSEPlus_ssse3_REF.c
new file mode 100644
index 000000000000..33d4eebadbe0
--- /dev/null
+++ b/arch/x86/kernel/SSEPlus_ssse3_REF.c
@@ -0,0 +1,319 @@
+#include <linux/kernel.h>
+
+#include "SSEPlus_REF.h"
+
+
+void ssp_abs_epi8(ssp_m128 *A)
+{
+	A->s8[0]  = (A->s8[0] < 0) ? -A->s8[0]  : A->s8[0];
+	A->s8[1]  = (A->s8[1] < 0) ? -A->s8[1]  : A->s8[1];
+	A->s8[2]  = (A->s8[2] < 0) ? -A->s8[2]  : A->s8[2];
+	A->s8[3]  = (A->s8[3] < 0) ? -A->s8[3]  : A->s8[3];
+	A->s8[4]  = (A->s8[4] < 0) ? -A->s8[4]  : A->s8[4];
+	A->s8[5]  = (A->s8[5] < 0) ? -A->s8[5]  : A->s8[5];
+	A->s8[6]  = (A->s8[6] < 0) ? -A->s8[6]  : A->s8[6];
+	A->s8[7]  = (A->s8[7] < 0) ? -A->s8[7]  : A->s8[7];
+	A->s8[8]  = (A->s8[8] < 0) ? -A->s8[8]  : A->s8[8];
+	A->s8[9]  = (A->s8[9] < 0) ? -A->s8[9]  : A->s8[9];
+	A->s8[10] = (A->s8[10] < 0) ? -A->s8[10] : A->s8[10];
+	A->s8[11] = (A->s8[11] < 0) ? -A->s8[11] : A->s8[11];
+	A->s8[12] = (A->s8[12] < 0) ? -A->s8[12] : A->s8[12];
+	A->s8[13] = (A->s8[13] < 0) ? -A->s8[13] : A->s8[13];
+	A->s8[14] = (A->s8[14] < 0) ? -A->s8[14] : A->s8[14];
+	A->s8[15] = (A->s8[15] < 0) ? -A->s8[15] : A->s8[15];
+}
+
+void ssp_abs_epi16(ssp_m128 *A)
+{
+	A->s16[0] = (A->s16[0] < 0) ? -A->s16[0]  : A->s16[0];
+	A->s16[1] = (A->s16[1] < 0) ? -A->s16[1]  : A->s16[1];
+	A->s16[2] = (A->s16[2] < 0) ? -A->s16[2]  : A->s16[2];
+	A->s16[3] = (A->s16[3] < 0) ? -A->s16[3]  : A->s16[3];
+	A->s16[4] = (A->s16[4] < 0) ? -A->s16[4]  : A->s16[4];
+	A->s16[5] = (A->s16[5] < 0) ? -A->s16[5]  : A->s16[5];
+	A->s16[6] = (A->s16[6] < 0) ? -A->s16[6]  : A->s16[6];
+	A->s16[7] = (A->s16[7] < 0) ? -A->s16[7]  : A->s16[7];
+}
+
+void ssp_abs_epi32(ssp_m128 *A)
+{
+	A->s32[0] = (A->s32[0] < 0) ? -A->s32[0]  : A->s32[0];
+	A->s32[1] = (A->s32[1] < 0) ? -A->s32[1]  : A->s32[1];
+	A->s32[2] = (A->s32[2] < 0) ? -A->s32[2]  : A->s32[2];
+	A->s32[3] = (A->s32[3] < 0) ? -A->s32[3]  : A->s32[3];
+}
+
+ssp_m128 ssp_shuffle_epi8(ssp_m128 *A, ssp_m128 *MSK)
+{
+	ssp_m128 B;
+
+	B.s8[0]  = (MSK->s8[0]  & 0x80) ? 0 : A->s8[(MSK->s8[0]  & 0xf)];
+	B.s8[1]  = (MSK->s8[1]  & 0x80) ? 0 : A->s8[(MSK->s8[1]  & 0xf)];
+	B.s8[2]  = (MSK->s8[2]  & 0x80) ? 0 : A->s8[(MSK->s8[2]  & 0xf)];
+	B.s8[3]  = (MSK->s8[3]  & 0x80) ? 0 : A->s8[(MSK->s8[3]  & 0xf)];
+	B.s8[4]  = (MSK->s8[4]  & 0x80) ? 0 : A->s8[(MSK->s8[4]  & 0xf)];
+	B.s8[5]  = (MSK->s8[5]  & 0x80) ? 0 : A->s8[(MSK->s8[5]  & 0xf)];
+	B.s8[6]  = (MSK->s8[6]  & 0x80) ? 0 : A->s8[(MSK->s8[6]  & 0xf)];
+	B.s8[7]  = (MSK->s8[7]  & 0x80) ? 0 : A->s8[(MSK->s8[7]  & 0xf)];
+	B.s8[8]  = (MSK->s8[8]  & 0x80) ? 0 : A->s8[(MSK->s8[8]  & 0xf)];
+	B.s8[9]  = (MSK->s8[9]  & 0x80) ? 0 : A->s8[(MSK->s8[9]  & 0xf)];
+	B.s8[10] = (MSK->s8[10] & 0x80) ? 0 : A->s8[(MSK->s8[10] & 0xf)];
+	B.s8[11] = (MSK->s8[11] & 0x80) ? 0 : A->s8[(MSK->s8[11] & 0xf)];
+	B.s8[12] = (MSK->s8[12] & 0x80) ? 0 : A->s8[(MSK->s8[12] & 0xf)];
+	B.s8[13] = (MSK->s8[13] & 0x80) ? 0 : A->s8[(MSK->s8[13] & 0xf)];
+	B.s8[14] = (MSK->s8[14] & 0x80) ? 0 : A->s8[(MSK->s8[14] & 0xf)];
+	B.s8[15] = (MSK->s8[15] & 0x80) ? 0 : A->s8[(MSK->s8[15] & 0xf)];
+
+	return B;
+}
+
+void ssp_alignr_epi8(ssp_m128 *ret, ssp_m128 *a, ssp_m128 *b, const unsigned int ralign)
+{
+	u8 tmp[32];
+	int i, j;
+
+	if (ralign == 0) {
+		*ret = *b;
+		return;
+	}
+
+	ret->u64[1] = ret->u64[0] = 0;
+
+	if (ralign >= 32)
+		return;
+
+	*((ssp_m128 *)(&tmp[0])) = *b;
+	*((ssp_m128 *)(&tmp[16])) = *a;
+
+	for (i = 15 + ralign, j = 15; i >= ralign; i--, j--)
+		ret->u8[j] = (i < 32) ? tmp[i] : 0;
+}
+
+ssp_m128 ssp_sign_epi8(ssp_m128* a, ssp_m128* b)
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.s8[0]  = (B.s8[0]<0)  ? (-A.s8[0])  :((B.s8[0]==0) ? 0: A.s8[0]);
+	A.s8[1]  = (B.s8[1]<0)  ? (-A.s8[1])  :((B.s8[1]==0) ? 0: A.s8[1]);
+	A.s8[2]  = (B.s8[2]<0)  ? (-A.s8[2])  :((B.s8[2]==0) ? 0: A.s8[2]);
+	A.s8[3]  = (B.s8[3]<0)  ? (-A.s8[3])  :((B.s8[3]==0) ? 0: A.s8[3]);
+	A.s8[4]  = (B.s8[4]<0)  ? (-A.s8[4])  :((B.s8[4]==0) ? 0: A.s8[4]);
+	A.s8[5]  = (B.s8[5]<0)  ? (-A.s8[5])  :((B.s8[5]==0) ? 0: A.s8[5]);
+	A.s8[6]  = (B.s8[6]<0)  ? (-A.s8[6])  :((B.s8[6]==0) ? 0: A.s8[6]);
+	A.s8[7]  = (B.s8[7]<0)  ? (-A.s8[7])  :((B.s8[7]==0) ? 0: A.s8[7]);
+	A.s8[8]  = (B.s8[8]<0)  ? (-A.s8[8])  :((B.s8[8]==0) ? 0: A.s8[8]);
+	A.s8[9]  = (B.s8[9]<0)  ? (-A.s8[9])  :((B.s8[9]==0) ? 0: A.s8[9]);
+	A.s8[10] = (B.s8[10]<0) ? (-A.s8[10]) :((B.s8[10]==0)? 0: A.s8[10]);
+	A.s8[11] = (B.s8[11]<0) ? (-A.s8[11]) :((B.s8[11]==0)? 0: A.s8[11]);
+	A.s8[12] = (B.s8[12]<0) ? (-A.s8[12]) :((B.s8[12]==0)? 0: A.s8[12]);
+	A.s8[13] = (B.s8[13]<0) ? (-A.s8[13]) :((B.s8[13]==0)? 0: A.s8[13]);
+	A.s8[14] = (B.s8[14]<0) ? (-A.s8[14]) :((B.s8[14]==0)? 0: A.s8[14]);
+	A.s8[15] = (B.s8[15]<0) ? (-A.s8[15]) :((B.s8[15]==0)? 0: A.s8[15]);
+
+	return A;
+}
+
+#define SSP_SATURATION(a, pos_limit, neg_limit) (a>pos_limit) ? pos_limit : ((a<neg_limit)?neg_limit:a)
+
+ssp_m128 ssp_sign_epi16(ssp_m128* a, ssp_m128* b)
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.s16[0]  = (B.s16[0]<0)  ? (-A.s16[0])  :((B.s16[0]==0) ? 0: A.s16[0]);
+	A.s16[1]  = (B.s16[1]<0)  ? (-A.s16[1])  :((B.s16[1]==0) ? 0: A.s16[1]);
+	A.s16[2]  = (B.s16[2]<0)  ? (-A.s16[2])  :((B.s16[2]==0) ? 0: A.s16[2]);
+	A.s16[3]  = (B.s16[3]<0)  ? (-A.s16[3])  :((B.s16[3]==0) ? 0: A.s16[3]);
+	A.s16[4]  = (B.s16[4]<0)  ? (-A.s16[4])  :((B.s16[4]==0) ? 0: A.s16[4]);
+	A.s16[5]  = (B.s16[5]<0)  ? (-A.s16[5])  :((B.s16[5]==0) ? 0: A.s16[5]);
+	A.s16[6]  = (B.s16[6]<0)  ? (-A.s16[6])  :((B.s16[6]==0) ? 0: A.s16[6]);
+	A.s16[7]  = (B.s16[7]<0)  ? (-A.s16[7])  :((B.s16[7]==0) ? 0: A.s16[7]);
+
+	return A;
+}
+
+ssp_m128 ssp_sign_epi32(ssp_m128* a, ssp_m128* b)
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.s32[0]  = (B.s32[0]<0)  ? (-A.s32[0])  :((B.s32[0]==0) ? 0: A.s32[0]);
+	A.s32[1]  = (B.s32[1]<0)  ? (-A.s32[1])  :((B.s32[1]==0) ? 0: A.s32[1]);
+	A.s32[2]  = (B.s32[2]<0)  ? (-A.s32[2])  :((B.s32[2]==0) ? 0: A.s32[2]);
+	A.s32[3]  = (B.s32[3]<0)  ? (-A.s32[3])  :((B.s32[3]==0) ? 0: A.s32[3]);
+
+	return A;
+}
+
+ssp_m128 ssp_mulhrs_epi16( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A,B;
+	A = *a;
+	B = *b;
+
+	A.s16[0] = (s16) ((A.s16[0] * B.s16[0] + 0x4000) >> 15);
+	A.s16[1] = (s16) ((A.s16[1] * B.s16[1] + 0x4000) >> 15);
+	A.s16[2] = (s16) ((A.s16[2] * B.s16[2] + 0x4000) >> 15);
+	A.s16[3] = (s16) ((A.s16[3] * B.s16[3] + 0x4000) >> 15);
+	A.s16[4] = (s16) ((A.s16[4] * B.s16[4] + 0x4000) >> 15);
+	A.s16[5] = (s16) ((A.s16[5] * B.s16[5] + 0x4000) >> 15);
+	A.s16[6] = (s16) ((A.s16[6] * B.s16[6] + 0x4000) >> 15);
+	A.s16[7] = (s16) ((A.s16[7] * B.s16[7] + 0x4000) >> 15);
+
+	return A;
+}
+
+ssp_m128 ssp_maddubs_epi16( ssp_m128* a,  ssp_m128* b)
+{
+	ssp_m128 A, B, C;
+	int tmp[8];
+	A = *a;
+	B = *b;
+
+	// a is 8 bit unsigned integer, b is signed integer
+	tmp[0] = A.u8[0] * B.s8[0] +  A.u8[1] * B.s8[1];
+	C.s16[0] = (s16)(SSP_SATURATION(tmp[0], 32767, -32768));
+
+	tmp[1] = A.u8[2] * B.s8[2] +  A.u8[3] * B.s8[3];
+	C.s16[1] = (s16)(SSP_SATURATION(tmp[1], 32767, -32768));
+
+	tmp[2] = A.u8[4] * B.s8[4] +  A.u8[5] * B.s8[5];
+	C.s16[2] = (s16)(SSP_SATURATION(tmp[2], 32767, -32768));
+
+	tmp[3] = A.u8[6] * B.s8[6] +  A.u8[7] * B.s8[7];
+	C.s16[3] = (s16)(SSP_SATURATION(tmp[3], 32767, -32768));
+
+	tmp[4] = A.u8[8] * B.s8[8] +  A.u8[9] * B.s8[9];
+	C.s16[4] = (s16)(SSP_SATURATION(tmp[4], 32767, -32768));
+
+	tmp[5] = A.u8[10] * B.s8[10] +  A.u8[11] * B.s8[11];
+	C.s16[5] = (s16)(SSP_SATURATION(tmp[5], 32767, -32768));
+
+	tmp[6] = A.u8[12] * B.s8[12] +  A.u8[13] * B.s8[13];
+	C.s16[6] = (s16)(SSP_SATURATION(tmp[6], 32767, -32768));
+
+	tmp[7] = A.u8[14] * B.s8[14] +  A.u8[15] * B.s8[15];
+	C.s16[7] = (s16)(SSP_SATURATION(tmp[7], 32767, -32768));
+
+	return C;
+}
+
+ssp_m128 ssp_hsub_epi16( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.s16[0] = A.s16[0] - A.s16[1];
+	A.s16[1] = A.s16[2] - A.s16[3];
+	A.s16[2] = A.s16[4] - A.s16[5];
+	A.s16[3] = A.s16[6] - A.s16[7];
+	A.s16[4] = B.s16[0] - B.s16[1];
+	A.s16[5] = B.s16[2] - B.s16[3];
+	A.s16[6] = B.s16[4] - B.s16[5];
+	A.s16[7] = B.s16[6] - B.s16[7];
+
+	return A;
+}
+
+ssp_m128 ssp_hsub_epi32( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.s32[0] = A.s32[0] - A.s32[1];
+	A.s32[1] = A.s32[2] - A.s32[3];
+	A.s32[2] = B.s32[0] - B.s32[1];
+	A.s32[3] = B.s32[2] - B.s32[3];
+
+	return A;
+}
+ssp_m128 ssp_hsubs_epi16( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A, B;
+	int answer[8];
+	A = *a;
+	B = *b;
+
+	answer[0] = A.s16[0] - A.s16[1];
+	A.s16[0]  = (s16) (SSP_SATURATION(answer[0], 32767, -32768));
+	answer[1] = A.s16[2] - A.s16[3];
+	A.s16[1]  = (s16) (SSP_SATURATION(answer[1], 32767, -32768));
+	answer[2] = A.s16[4] - A.s16[5];
+	A.s16[2]  = (s16) (SSP_SATURATION(answer[2], 32767, -32768));
+	answer[3] = A.s16[6] - A.s16[7];
+	A.s16[3]  = (s16) (SSP_SATURATION(answer[3], 32767, -32768));
+	answer[4] = B.s16[0] - B.s16[1];
+	A.s16[4]  = (s16) (SSP_SATURATION(answer[4], 32767, -32768));
+	answer[5] = B.s16[2] - B.s16[3];
+	A.s16[5]  = (s16) (SSP_SATURATION(answer[5], 32767, -32768));
+	answer[6] = B.s16[4] - B.s16[5];
+	A.s16[6]  = (s16) (SSP_SATURATION(answer[6], 32767, -32768));
+	answer[7] = B.s16[6] - B.s16[7];
+	A.s16[7]  = (s16) (SSP_SATURATION(answer[7], 32767, -32768));
+
+	return A;
+}
+
+ssp_m128 ssp_hadd_epi16( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.s16[0] = A.s16[0] + A.s16[1];
+	A.s16[1] = A.s16[2] + A.s16[3];
+	A.s16[2] = A.s16[4] + A.s16[5];
+	A.s16[3] = A.s16[6] + A.s16[7];
+	A.s16[4] = B.s16[0] + B.s16[1];
+	A.s16[5] = B.s16[2] + B.s16[3];
+	A.s16[6] = B.s16[4] + B.s16[5];
+	A.s16[7] = B.s16[6] + B.s16[7];
+	return A;
+}
+
+ssp_m128 ssp_hadd_epi32( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A, B;
+	A = *a;
+	B = *b;
+
+	A.s32[0] = A.s32[0] + A.s32[1];
+	A.s32[1] = A.s32[2] + A.s32[3];
+	A.s32[2] = B.s32[0] + B.s32[1];
+	A.s32[3] = B.s32[2] + B.s32[3];
+
+	return A;
+}
+
+ssp_m128 ssp_hadds_epi16( ssp_m128* a, ssp_m128* b )
+{
+	ssp_m128 A, B;
+	int answer[8];
+	A = *a;
+	B = *b;
+
+	answer[0] = A.s16[0] + A.s16[1];
+	A.s16[0]  = (s16) (SSP_SATURATION(answer[0], 32767, -32768));
+	answer[1] = A.s16[2] + A.s16[3];
+	A.s16[1]  = (s16) (SSP_SATURATION(answer[1], 32767, -32768));
+	answer[2] = A.s16[4] + A.s16[5];
+	A.s16[2]  = (s16) (SSP_SATURATION(answer[2], 32767, -32768));
+	answer[3] = A.s16[6] + A.s16[7];
+	A.s16[3]  = (s16) (SSP_SATURATION(answer[3], 32767, -32768));
+	answer[4] = B.s16[0] + B.s16[1];
+	A.s16[4]  = (s16) (SSP_SATURATION(answer[4], 32767, -32768));
+	answer[5] = B.s16[2] + B.s16[3];
+	A.s16[5]  = (s16) (SSP_SATURATION(answer[5], 32767, -32768));
+	answer[6] = B.s16[4] + B.s16[5];
+	A.s16[6]  = (s16) (SSP_SATURATION(answer[6], 32767, -32768));
+	answer[7] = B.s16[6] + B.s16[7];
+	A.s16[7]  = (s16) (SSP_SATURATION(answer[7], 32767, -32768));
+
+	return A;
+}
diff --git a/arch/x86/kernel/SSEPlus_utils.c b/arch/x86/kernel/SSEPlus_utils.c
new file mode 100644
index 000000000000..69c406afd043
--- /dev/null
+++ b/arch/x86/kernel/SSEPlus_utils.c
@@ -0,0 +1,230 @@
+#include <linux/kernel.h>
+
+#include <asm/stacktrace.h>
+#include <asm/processor.h>
+
+#include "SSEPlus_REF.h"
+
+ssp_m128 getXMMRegister(int index, int extended)
+{
+	u8 buf[32];
+	ssp_m128 *alignedValue = (ssp_m128 *)ALIGN((unsigned long)buf, 16);
+
+#ifdef CONFIG_X86_64
+	if (extended)
+		switch (index) {
+		case 0:
+			asm volatile("movdqa %%xmm8, %0" : "=m"(*alignedValue));
+			break;
+		case 1:
+			asm volatile("movdqa %%xmm9, %0" : "=m"(*alignedValue));
+			break;
+		case 2:
+			asm volatile("movdqa %%xmm10, %0" : "=m"(*alignedValue));
+			break;
+		case 3:
+			asm volatile("movdqa %%xmm11, %0" : "=m"(*alignedValue));
+			break;
+		case 4:
+			asm volatile("movdqa %%xmm12, %0" : "=m"(*alignedValue));
+			break;
+		case 5:
+			asm volatile("movdqa %%xmm13, %0" : "=m"(*alignedValue));
+			break;
+		case 6:
+			asm volatile("movdqa %%xmm14, %0" : "=m"(*alignedValue));
+			break;
+		case 7:
+			asm volatile("movdqa %%xmm15, %0" : "=m"(*alignedValue));
+			break;
+		}
+	else
+#endif
+	switch (index) {
+	case 0:
+		asm volatile("movdqa %%xmm0, %0" : "=m"(*alignedValue));
+		break;
+	case 1:
+		asm volatile("movdqa %%xmm1, %0" : "=m"(*alignedValue));
+		break;
+	case 2:
+		asm volatile("movdqa %%xmm2, %0" : "=m"(*alignedValue));
+		break;
+	case 3:
+		asm volatile("movdqa %%xmm3, %0" : "=m"(*alignedValue));
+		break;
+	case 4:
+		asm volatile("movdqa %%xmm4, %0" : "=m"(*alignedValue));
+		break;
+	case 5:
+		asm volatile("movdqa %%xmm5, %0" : "=m"(*alignedValue));
+		break;
+	case 6:
+		asm volatile("movdqa %%xmm6, %0" : "=m"(*alignedValue));
+		break;
+	case 7:
+		asm volatile("movdqa %%xmm7, %0" : "=m"(*alignedValue));
+		break;
+	}
+
+	return *alignedValue;
+}
+
+void setXMMRegister(int index, int extended, ssp_m128* value)
+{
+	u8 buf[32];
+	ssp_m128 *alignedValue = (ssp_m128 *)ALIGN((unsigned long)buf, 16);
+	*alignedValue = *value;
+
+#ifdef CONFIG_X86_64
+	if (extended)
+		switch (index) {
+		case 0:
+			asm volatile("movdqa %0, %%xmm8" : : "m" (*alignedValue));
+			break;
+		case 1:
+			asm volatile("movdqa %0, %%xmm9" : : "m" (*alignedValue));
+			break;
+		case 2:
+			asm volatile("movdqa %0, %%xmm10" : : "m" (*alignedValue));
+			break;
+		case 3:
+			asm volatile("movdqa %0, %%xmm11" : : "m" (*alignedValue));
+			break;
+		case 4:
+			asm volatile("movdqa %0, %%xmm12" : : "m" (*alignedValue));
+			break;
+		case 5:
+			asm volatile("movdqa %0, %%xmm13" : : "m" (*alignedValue));
+			break;
+		case 6:
+			asm volatile("movdqa %0, %%xmm14" : : "m" (*alignedValue));
+			break;
+		case 7:
+			asm volatile("movdqa %0, %%xmm15" : : "m" (*alignedValue));
+			break;
+		}
+	else
+#endif
+	switch (index) {
+	case 0:
+		asm volatile("movdqa %0, %%xmm0" : : "m" (*alignedValue));
+		break;
+	case 1:
+		asm volatile("movdqa %0, %%xmm1" : : "m" (*alignedValue));
+		break;
+	case 2:
+		asm volatile("movdqa %0, %%xmm2" : : "m" (*alignedValue));
+		break;
+	case 3:
+		asm volatile("movdqa %0, %%xmm3" : : "m" (*alignedValue));
+		break;
+	case 4:
+		asm volatile("movdqa %0, %%xmm4" : : "m" (*alignedValue));
+		break;
+	case 5:
+		asm volatile("movdqa %0, %%xmm5" : : "m" (*alignedValue));
+		break;
+	case 6:
+		asm volatile("movdqa %0, %%xmm6" : : "m" (*alignedValue));
+		break;
+	case 7:
+		asm volatile("movdqa %0, %%xmm7" : : "m" (*alignedValue));
+		break;
+	}
+}
+
+unsigned long* getRegisterPtr(int index, struct pt_regs* regs, int extended) {
+	unsigned long* regTable1[] = {
+		&regs->ax, &regs->cx,
+		&regs->dx, &regs->bx,
+		&regs->sp, &regs->bp,
+		&regs->si, &regs->di,
+	};
+	unsigned long** regTable = regTable1;
+#ifdef CONFIG_X86_64
+	unsigned long* regTable2[] = {
+		&regs->r8, &regs->r9,
+		&regs->r10, &regs->r11,
+		&regs->r12, &regs->r13,
+		&regs->r14, &regs->r15,
+	};
+	if (extended) {
+		regTable = regTable2;
+	}
+#endif
+	return regTable[index];
+}
+
+void setRegister(int index, struct pt_regs* regs, int extended, unsigned long value) {
+	*getRegisterPtr(index, regs, extended) = value;
+}
+
+int decodeMemAddress(int opcode, struct pt_regs* regs, int rex, u8* extraBytes, unsigned long *memAddr) {
+	int extraLength;
+	int srcIndex = opcode & 0x7;
+	if (srcIndex == 0x4) {
+		int opHigh = (extraBytes[0]>>3) & 0x7;
+		int opLow = extraBytes[0] & 0x7;
+		if (opHigh == 0x4 && opLow == 0x04) {
+			*memAddr = *getRegisterPtr(opLow, regs, testREX(rex, REX_B));
+		} else {
+			int multiplier = 1 << (extraBytes[0]>>6);
+			*memAddr = *getRegisterPtr(opLow, regs, testREX(rex, REX_B)) +
+						*getRegisterPtr(opHigh, regs, testREX(rex, REX_X)) * multiplier;
+		}
+		extraLength = 1;
+	} else if (srcIndex == 0x5 && opcode < 0x40) {
+		*memAddr = *(u32*)&extraBytes[0];
+		extraLength = 4;
+	} else {
+		*memAddr = *getRegisterPtr(srcIndex, regs, testREX(rex, REX_B));
+		extraLength = 0;
+	}
+
+	if (opcode & 0x40) {
+		*memAddr += (s8)extraBytes[extraLength];
+		extraLength += 1;
+	} else if (opcode & 0x80) {
+		*memAddr += *(s64*)&extraBytes[extraLength];
+		extraLength += 4;
+	}
+
+	return extraLength;
+}
+
+int getOp2MemValue(int opcode, struct pt_regs* regs, int rex, u8* extraBytes, unsigned long* value) {
+	if (opcode >= 0xc0) {
+		*value = *getRegisterPtr(opcode & 0x7, regs, testREX(rex, REX_B));
+		return 0;
+	} else {
+		unsigned long memAddr = 0;
+		u8 data[sizeof(unsigned long)] = {0};
+		int extraLen = decodeMemAddress(opcode, regs, rex, extraBytes, &memAddr);
+
+		if (memAddr) {
+			copy_from_user((void *)data, (const void __user *)memAddr, sizeof(unsigned long));
+			*value = *(unsigned long*)data;
+			return extraLen;
+		}
+	}
+	return -1;
+}
+
+int getOp2XMMValue(int opcode, struct pt_regs* regs, int rex, u8* extraBytes, ssp_m128* value) {
+	if (opcode >= 0xc0) {
+		*value = getXMMRegister(opcode & 0x7, testREX(rex, REX_B));
+		return 0;
+	} else {
+		unsigned long memAddr = 0;
+		u8 data[sizeof(ssp_m128)] = {0};
+		int extraLen = decodeMemAddress(opcode, regs, rex, extraBytes, &memAddr);
+
+		if (memAddr) {
+			copy_from_user((void *)data, (const void __user *)memAddr, sizeof(ssp_m128));
+			*value = *(ssp_m128*)data;
+			return extraLen;
+		}
+	}
+	return -1;
+}
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 6fbf0a178aa4..83e37b9c777a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -71,6 +71,8 @@
 #include <asm/proto.h>
 #endif
 
+#include "SSEPlus_REF.h"
+
 DECLARE_BITMAP(system_vectors, NR_VECTORS);
 
 static inline void cond_local_irq_enable(struct pt_regs *regs)
@@ -209,105 +211,14 @@ DEFINE_IDTENTRY(exc_overflow)
 	do_error_trap(regs, 0, "overflow", X86_TRAP_OF, SIGSEGV, 0, NULL);
 }
 
-typedef union {
-	u64 u64[2];
-	s64 s64[2];
-	u32 u32[4];
-	s32 s32[4];
-	u16 u16[8];
-	s16 s16[8];
-	u8 u8[16];
-	s8 s8[16];
-} ssp_m128 __aligned(16);
-
-static void ssp_abs_epi8(ssp_m128 *A)
-{
-	A->s8[0]  = (A->s8[0] < 0) ? -A->s8[0]  : A->s8[0];
-	A->s8[1]  = (A->s8[1] < 0) ? -A->s8[1]  : A->s8[1];
-	A->s8[2]  = (A->s8[2] < 0) ? -A->s8[2]  : A->s8[2];
-	A->s8[3]  = (A->s8[3] < 0) ? -A->s8[3]  : A->s8[3];
-	A->s8[4]  = (A->s8[4] < 0) ? -A->s8[4]  : A->s8[4];
-	A->s8[5]  = (A->s8[5] < 0) ? -A->s8[5]  : A->s8[5];
-	A->s8[6]  = (A->s8[6] < 0) ? -A->s8[6]  : A->s8[6];
-	A->s8[7]  = (A->s8[7] < 0) ? -A->s8[7]  : A->s8[7];
-	A->s8[8]  = (A->s8[8] < 0) ? -A->s8[8]  : A->s8[8];
-	A->s8[9]  = (A->s8[9] < 0) ? -A->s8[9]  : A->s8[9];
-	A->s8[10] = (A->s8[10] < 0) ? -A->s8[10] : A->s8[10];
-	A->s8[11] = (A->s8[11] < 0) ? -A->s8[11] : A->s8[11];
-	A->s8[12] = (A->s8[12] < 0) ? -A->s8[12] : A->s8[12];
-	A->s8[13] = (A->s8[13] < 0) ? -A->s8[13] : A->s8[13];
-	A->s8[14] = (A->s8[14] < 0) ? -A->s8[14] : A->s8[14];
-	A->s8[15] = (A->s8[15] < 0) ? -A->s8[15] : A->s8[15];
-}
-
-static void ssp_abs_epi16(ssp_m128 *A)
-{
-	A->s16[0] = (A->s16[0] < 0) ? -A->s16[0]  : A->s16[0];
-	A->s16[1] = (A->s16[1] < 0) ? -A->s16[1]  : A->s16[1];
-	A->s16[2] = (A->s16[2] < 0) ? -A->s16[2]  : A->s16[2];
-	A->s16[3] = (A->s16[3] < 0) ? -A->s16[3]  : A->s16[3];
-	A->s16[4] = (A->s16[4] < 0) ? -A->s16[4]  : A->s16[4];
-	A->s16[5] = (A->s16[5] < 0) ? -A->s16[5]  : A->s16[5];
-	A->s16[6] = (A->s16[6] < 0) ? -A->s16[6]  : A->s16[6];
-	A->s16[7] = (A->s16[7] < 0) ? -A->s16[7]  : A->s16[7];
-}
-
-static void ssp_abs_epi32(ssp_m128 *A)
-{
-	A->s32[0] = (A->s32[0] < 0) ? -A->s32[0]  : A->s32[0];
-	A->s32[1] = (A->s32[1] < 0) ? -A->s32[1]  : A->s32[1];
-	A->s32[2] = (A->s32[2] < 0) ? -A->s32[2]  : A->s32[2];
-	A->s32[3] = (A->s32[3] < 0) ? -A->s32[3]  : A->s32[3];
-}
-
-static ssp_m128 ssp_shuffle_epi8(ssp_m128 *A, ssp_m128 *MSK)
-{
-	ssp_m128 B;
-
-	B.s8[0]  = (MSK->s8[0]  & 0x80) ? 0 : A->s8[(MSK->s8[0]  & 0xf)];
-	B.s8[1]  = (MSK->s8[1]  & 0x80) ? 0 : A->s8[(MSK->s8[1]  & 0xf)];
-	B.s8[2]  = (MSK->s8[2]  & 0x80) ? 0 : A->s8[(MSK->s8[2]  & 0xf)];
-	B.s8[3]  = (MSK->s8[3]  & 0x80) ? 0 : A->s8[(MSK->s8[3]  & 0xf)];
-	B.s8[4]  = (MSK->s8[4]  & 0x80) ? 0 : A->s8[(MSK->s8[4]  & 0xf)];
-	B.s8[5]  = (MSK->s8[5]  & 0x80) ? 0 : A->s8[(MSK->s8[5]  & 0xf)];
-	B.s8[6]  = (MSK->s8[6]  & 0x80) ? 0 : A->s8[(MSK->s8[6]  & 0xf)];
-	B.s8[7]  = (MSK->s8[7]  & 0x80) ? 0 : A->s8[(MSK->s8[7]  & 0xf)];
-	B.s8[8]  = (MSK->s8[8]  & 0x80) ? 0 : A->s8[(MSK->s8[8]  & 0xf)];
-	B.s8[9]  = (MSK->s8[9]  & 0x80) ? 0 : A->s8[(MSK->s8[9]  & 0xf)];
-	B.s8[10] = (MSK->s8[10] & 0x80) ? 0 : A->s8[(MSK->s8[10] & 0xf)];
-	B.s8[11] = (MSK->s8[11] & 0x80) ? 0 : A->s8[(MSK->s8[11] & 0xf)];
-	B.s8[12] = (MSK->s8[12] & 0x80) ? 0 : A->s8[(MSK->s8[12] & 0xf)];
-	B.s8[13] = (MSK->s8[13] & 0x80) ? 0 : A->s8[(MSK->s8[13] & 0xf)];
-	B.s8[14] = (MSK->s8[14] & 0x80) ? 0 : A->s8[(MSK->s8[14] & 0xf)];
-	B.s8[15] = (MSK->s8[15] & 0x80) ? 0 : A->s8[(MSK->s8[15] & 0xf)];
-
-	return B;
-}
-
-static void ssp_alignr_epi8(ssp_m128 *ret, ssp_m128 *a, ssp_m128 *b,
-			     const unsigned int ralign)
-{
-	u8 tmp[32];
-	int i, j;
-
-	if (ralign == 0) {
-		*ret = *b;
-		return;
-	}
-
-	ret->u64[1] = ret->u64[0] = 0;
-
-	if (ralign >= 32)
-		return;
+#define OPCODE_SIZE 12
+#define DEBUG_INST_EMULATION 0
 
-	*((ssp_m128 *)(&tmp[0])) = *b;
-	*((ssp_m128 *)(&tmp[16])) = *a;
-
-	for (i = 15 + ralign, j = 15; i >= ralign; i--, j--)
-		ret->u8[j] = (i < 32) ? tmp[i] : 0;
-}
-
-#define OPCODE_SIZE 6
+#if DEBUG_INST_EMULATION
+#define INSTR_NAME(x) __instr_name = x
+#else
+#define INSTR_NAME(x)
+#endif
 
 void do_invalid_op(struct pt_regs *regs, long error_code)
 {
@@ -317,6 +228,10 @@ void do_invalid_op(struct pt_regs *regs, long error_code)
 	union {
 		unsigned char byte[OPCODE_SIZE];
 	} opcode;
+	int prefix66 = 0, prefixREX = 0;
+#if DEBUG_INST_EMULATION
+	const char* __instr_name = NULL;
+#endif
 
 	info.si_signo = SIGILL;
 	info.si_errno = 0;
@@ -330,8 +245,18 @@ void do_invalid_op(struct pt_regs *regs, long error_code)
 		pr_info("No user code available.");
 	}
 
-	if (opcode.byte[0] == 0x66) {
+	// 0xf3 prefix is used by popcnt
+	if (opcode.byte[0] == 0x66 || opcode.byte[0] == 0xf3) {
 		int i;
+		prefix66 = opcode.byte[0] == 0x66;
+		for (i = 1; i < OPCODE_SIZE; i++)
+			opcode.byte[i-1] = opcode.byte[i];
+		regs->ip++;
+	}
+
+	while ((opcode.byte[0] & 0xf0) == 0x40) {
+		int i;
+		prefixREX = opcode.byte[0];
 		for (i = 1; i < OPCODE_SIZE; i++)
 			opcode.byte[i-1] = opcode.byte[i];
 		regs->ip++;
@@ -339,114 +264,566 @@ void do_invalid_op(struct pt_regs *regs, long error_code)
 
 	if (opcode.byte[0] == 0x0f) {
 		if (opcode.byte[1] == 0x38) {
-			switch (opcode.byte[2]) {
-			case 0x00:
-				if (opcode.byte[3] == 0xc1) {
-					ssp_m128 ret;
-					ssp_m128 mask;
-					asm volatile("movdqa %%xmm0, %0" : "=m"(ret));
-					asm volatile("movdqa %%xmm1, %0" : "=m"(mask));
-					ret = ssp_shuffle_epi8(&ret, &mask);
-					asm volatile("movdqa %0, %%xmm0" : : "m"(ret));
-					regs->ip += 4;
+			ssp_m128 ret, src;
+			unsigned int dstIndex = (opcode.byte[3]>>3) & 0x7;
+			int op_len;
+
+			if (opcode.byte[2] == 0x2a) {
+				unsigned long memAddr = 0;
+				int regIndex = (opcode.byte[3]>>3) & 0x7;
+				int op_len = 4 + decodeMemAddress(opcode.byte[3], regs, prefixREX, &opcode.byte[4], &memAddr);
+				u8 data[sizeof(ssp_m128)];
+
+				INSTR_NAME("movntdqa");
+
+				if (memAddr && !copy_from_user((void *)data, (const void __user *)memAddr, sizeof(ssp_m128))) {
+					ssp_m128 ret = ssp_stream_load_si128((ssp_m128*)data);
+					setXMMRegister(regIndex, testREX(prefixREX, REX_R), &ret);
 					handled = 1;
+					regs->ip += op_len;
 				}
-				break;
-			case 0x1c:
-				if (opcode.byte[3] == 0xc8) {
-					ssp_m128 ret;
-					asm volatile("movdqa %%xmm0, %0" : "=m" (ret));
-					ssp_abs_epi8(&ret);
-					asm volatile("movdqa %0, %%xmm1" : : "m" (ret));
-					regs->ip += 4;
+			}
+			else if (opcode.byte[2] == 0xf0 || opcode.byte[2] == 0xf1) {
+				unsigned long memAddr = 0;
+				int regIndex = (opcode.byte[3]>>3) & 0x7;
+				int op_bytes = testREX(prefixREX, REX_W) ? 8 : (prefix66 ? 2 : 4);
+				int op_len = 4 + decodeMemAddress(opcode.byte[3], regs, prefixREX, &opcode.byte[4], &memAddr);
+				u8 data[8];
+
+				INSTR_NAME("movbe");
+
+				if (memAddr && opcode.byte[2] == 0xf0) {
+					// dst reg
+					if (!copy_from_user((void *)data, (const void __user *)memAddr, op_bytes)) {
+						unsigned long* regValue = getRegisterPtr(regIndex, regs, testREX(prefixREX, REX_R));
+						switch (op_bytes) {
+						case 2:
+							*regValue &= ~0xffffUL;
+							*regValue |= swab16(*(u16*)data);
+							break;
+						case 4:
+							*regValue &= ~0xffffffffUL;
+							*regValue |= swab32(*(u32*)data);
+							break;
+						case 8:
+							*regValue = swab64(*(u64*)data);
+							break;
+						}
+						handled = 1;
+						regs->ip += op_len;
+					}
+					else {
+						pr_info("movbe copy_from_user failed. op_bytes=%d, op_len=%d, memAddr=%p\n",
+								op_bytes, op_len, (void*)memAddr);
+					}
+				}
+				else if (memAddr) {
+					// dst mem
+					switch (op_bytes) {
+					case 2:
+						*(u16*)data = swab16(*(u16*)getRegisterPtr(regIndex, regs, testREX(prefixREX, REX_R)));
+						break;
+					case 4:
+						*(u32*)data = swab32(*(u32*)getRegisterPtr(regIndex, regs, testREX(prefixREX, REX_R)));
+						break;
+					case 8:
+						*(u64*)data = swab64(*(u64*)getRegisterPtr(regIndex, regs, testREX(prefixREX, REX_R)));
+						break;
+					}
+					if (!copy_to_user((void __user *)memAddr, (void *)data, op_bytes)) {
+						handled = 1;
+						regs->ip += op_len;
+					}
+					else {
+						pr_info("movbe copy_to_user failed. op_bytes=%d, op_len=%d, memAddr=%p\n",
+								op_bytes, op_len, (void*)memAddr);
+					}
+				}
+			}
+			else if ((op_len = getOp2XMMValue(opcode.byte[3], regs, prefixREX, &opcode.byte[4], &src)) != -1) {
+				op_len += 4;
+				ret = getXMMRegister(dstIndex, testREX(prefixREX, REX_R));
+
+				switch (opcode.byte[2]) {
+				case 0x00:
+					INSTR_NAME("pshufb");
+					ret = ssp_shuffle_epi8(&ret, &src);
+					handled = 1;
+					break;
+				case 0x01:
+					INSTR_NAME("phaddw");
+					ret = ssp_hadd_epi16(&ret, &src);
+					handled = 1;
+					break;
+				case 0x02:
+					INSTR_NAME("phaddd");
+					ret = ssp_hadd_epi32(&ret, &src);
+					handled = 1;
+					break;
+				case 0x03:
+					INSTR_NAME("phaddsw");
+					ret = ssp_hadds_epi16(&ret, &src);
+					handled = 1;
+					break;
+				case 0x04:
+					INSTR_NAME("pmaddubsw");
+					ret = ssp_maddubs_epi16(&ret, &src);
+					handled = 1;
+					break;
+				case 0x05:
+					INSTR_NAME("phsubw");
+					ret = ssp_hsub_epi16(&ret, &src);
+					handled = 1;
+					break;
+				case 0x06:
+					INSTR_NAME("phsubd");
+					ret = ssp_hsub_epi32(&ret, &src);
+					handled = 1;
+					break;
+				case 0x07:
+					INSTR_NAME("phsubsw");
+					ret = ssp_hsubs_epi16(&ret, &src);
+					handled = 1;
+					break;
+				case 0x08:
+					INSTR_NAME("psignb");
+					ret = ssp_sign_epi8(&ret, &src);
+					handled = 1;
+					break;
+				case 0x09:
+					INSTR_NAME("psignw");
+					ret = ssp_sign_epi16(&ret, &src);
+					handled = 1;
+					break;
+				case 0x0a:
+					INSTR_NAME("psignd");
+					ret = ssp_sign_epi32(&ret, &src);
 					handled = 1;
+					break;
+				case 0x0b:
+					INSTR_NAME("pmulhrsw");
+					ret = ssp_mulhrs_epi16(&ret, &src);
+					handled = 1;
+					break;
+				case 0x10:
+				{
+					ssp_m128 op3 = getXMMRegister(0, 0);
+					INSTR_NAME("pblendvb");
+					ret = ssp_blendv_epi8(&ret, &src, &op3);
+					handled = 1;
+					break;
 				}
-				break;
-			case 0x1d:
-				if (opcode.byte[3] == 0xc8) {
-					ssp_m128 ret;
-					asm volatile("movdqa %%xmm0, %0" : "=m" (ret));
-					ssp_abs_epi16(&ret);
-					asm volatile("movdqa %0, %%xmm1" : : "m" (ret));
-					regs->ip += 4;
+				case 0x14:
+				{
+					ssp_m128 op3 = getXMMRegister(0, 0);
+					INSTR_NAME("blendvps");
+					ret = ssp_blendv_ps(&ret, &src, &op3);
 					handled = 1;
+					break;
 				}
-				break;
-			case 0x1e:
-				if (opcode.byte[3] == 0xc8) {
-					ssp_m128 ret;
-					asm volatile("movdqa %%xmm0, %0" : "=m" (ret));
+				case 0x15:
+				{
+					ssp_m128 op3 = getXMMRegister(0, 0);
+					INSTR_NAME("blendvpd");
+					ret = ssp_blendv_pd(&ret, &src, &op3);
+					handled = 1;
+					break;
+				}
+				case 0x17:
+				{
+					int cf = ssp_testc_si128(&ret, &src);
+					int zf = ssp_testz_si128(&ret, &src);
+					INSTR_NAME("ptest");
+					if (zf) regs->flags |= 1<<6;
+					if (cf) regs->flags |= 1;
+					handled = 1;
+					break;
+				}
+				case 0x1c:
+					INSTR_NAME("pabsb");
+					ret = src;
+					ssp_abs_epi8(&ret);
+					handled = 1;
+					break;
+				case 0x1d:
+					INSTR_NAME("pabsw");
+					ret = src;
+					ssp_abs_epi16(&ret);
+					handled = 1;
+					break;
+				case 0x1e:
+					INSTR_NAME("pabsd");
+					ret = src;
 					ssp_abs_epi32(&ret);
-					asm volatile("movdqa %0, %%xmm1" : : "m" (ret));
-					regs->ip += 4;
 					handled = 1;
+					break;
+				case 0x20:
+					INSTR_NAME("pmovsxbw");
+					ret = ssp_cvtepi8_epi16(&src);
+					handled = 1;
+					break;
+				case 0x21:
+					INSTR_NAME("pmovsxbd");
+					ret = ssp_cvtepi8_epi32(&src);
+					handled = 1;
+					break;
+				case 0x22:
+					INSTR_NAME("pmovsxbq");
+					ret = ssp_cvtepi8_epi64(&src);
+					handled = 1;
+					break;
+				case 0x23:
+					INSTR_NAME("pmovsxwd");
+					ret = ssp_cvtepi16_epi32(&src);
+					handled = 1;
+					break;
+				case 0x24:
+					INSTR_NAME("pmovsxwq");
+					ret = ssp_cvtepi16_epi64(&src);
+					handled = 1;
+					break;
+				case 0x25:
+					INSTR_NAME("pmovsxdq");
+					ret = ssp_cvtepi32_epi64(&src);
+					handled = 1;
+					break;
+				case 0x28:
+					INSTR_NAME("pmuldq");
+					ret = ssp_mul_epi32(&ret, &src);
+					handled = 1;
+					break;
+				case 0x29:
+					INSTR_NAME("pcmpeqq");
+					ret = ssp_cmpeq_epi64(&ret, &src);
+					handled = 1;
+					break;
+				case 0x2b:
+					INSTR_NAME("packusdw");
+					ret = ssp_packus_epi32(&ret, &src);
+					handled = 1;
+					break;
+				case 0x30:
+					INSTR_NAME("pmovzxbw");
+					ret = ssp_cvtepu8_epi16(&src);
+					handled = 1;
+					break;
+				case 0x31:
+					INSTR_NAME("pmovzxbd");
+					ret = ssp_cvtepu8_epi32(&src);
+					handled = 1;
+					break;
+				case 0x32:
+					INSTR_NAME("pmovzxbq");
+					ret = ssp_cvtepu8_epi64(&src);
+					handled = 1;
+					break;
+				case 0x33:
+					INSTR_NAME("pmovzxwd");
+					ret = ssp_cvtepu16_epi32(&src);
+					handled = 1;
+					break;
+				case 0x34:
+					INSTR_NAME("pmovzxwq");
+					ret = ssp_cvtepu16_epi64(&src);
+					handled = 1;
+					break;
+				case 0x35:
+					INSTR_NAME("pmovzxdq");
+					ret = ssp_cvtepu32_epi64(&src);
+					handled = 1;
+					break;
+				case 0x38:
+					INSTR_NAME("pminsb");
+					ret = ssp_min_epi8(&src, &ret);
+					handled = 1;
+					break;
+				case 0x39:
+					INSTR_NAME("pminsd");
+					ret = ssp_min_epi32(&src, &ret);
+					handled = 1;
+					break;
+				case 0x3a:
+					INSTR_NAME("pminuw");
+					ret = ssp_min_epu16(&src, &ret);
+					handled = 1;
+					break;
+				case 0x3b:
+					INSTR_NAME("pminud");
+					ret = ssp_min_epu32(&src, &ret);
+					handled = 1;
+					break;
+				case 0x3c:
+					INSTR_NAME("pmaxsb");
+					ret = ssp_max_epi8(&src, &ret);
+					handled = 1;
+					break;
+				case 0x3d:
+					INSTR_NAME("pmaxsd");
+					ret = ssp_max_epi32(&src, &ret);
+					handled = 1;
+					break;
+				case 0x3e:
+					INSTR_NAME("pmaxuw");
+					ret = ssp_max_epu16(&src, &ret);
+					handled = 1;
+					break;
+				case 0x3f:
+					INSTR_NAME("pmaxud");
+					ret = ssp_max_epu32(&src, &ret);
+					handled = 1;
+					break;
+				case 0x40:
+					INSTR_NAME("pmulld");
+					ret = ssp_mullo_epi32(&ret, &src);
+					handled = 1;
+					break;
+				case 0x41:
+					INSTR_NAME("phminposuw");
+					ret = ssp_minpos_epu16(&src);
+					handled = 1;
+					break;
+				}
+
+				if (handled) {
+					setXMMRegister(dstIndex, testREX(prefixREX, REX_R), &ret);
+					regs->ip += op_len;
+				}
+			}
+		}
+		else if (opcode.byte[1] == 0x3a) {
+			ssp_m128 a, b, ret;
+			int op_len, immValue;
+
+			unsigned int aIndex = (opcode.byte[3]>>3) & 0x7;;
+			a = getXMMRegister(aIndex, testREX(prefixREX, REX_R));
+
+			// PINSRB family
+			unsigned long memValue;
+			if ((opcode.byte[2] == 0x20 || opcode.byte[2] == 0x22) &&
+				((op_len = getOp2MemValue(opcode.byte[3], regs, prefixREX, &opcode.byte[4], &memValue)) != -1)) {
+				immValue = opcode.byte[4 + op_len];
+				op_len += 5;
+
+				switch (opcode.byte[2]) {
+				case 0x20:
+					INSTR_NAME("pinsrb");
+					ret = ssp_insert_epi8(&a, memValue, immValue);
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					handled = 1;
+					break;
+				case 0x22:
+					if (testREX(prefixREX, REX_W)) {
+						INSTR_NAME("pinsrq");
+						ret = ssp_insert_epi64(&a, memValue, immValue);
+					}
+					else {
+						INSTR_NAME("pinsrd");
+						ret = ssp_insert_epi32(&a, memValue, immValue);
+					}
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					handled = 1;
+					break;
 				}
-				break;
 			}
-		} else if ((opcode.byte[1] == 0x3a) && (opcode.byte[2] == 0x0f)) {
-			ssp_m128 ret;
-			ssp_m128 a;
-			ssp_m128 b;
-			int ralign;
 
-			ralign = opcode.byte[4];
+			// EXTRACTPS/PEXTRB family
+			if (!handled && (opcode.byte[2] == 0x14 || opcode.byte[2] == 0x16 || opcode.byte[2] == 0x17)) {
+				s64 extractValue;
+				unsigned long memAddr = 0;
+				int regIndex = 0;
+				int dstLength = 0;
+				if (opcode.byte[3] >= 0xc0) {
+					immValue = opcode.byte[4];
+					op_len = 5;
+					regIndex = opcode.byte[3] & 0x7;
+				}
+				else {
+					op_len = decodeMemAddress(opcode.byte[3], regs, prefixREX, &opcode.byte[4], &memAddr);
+					if (op_len != -1) {
+						immValue = opcode.byte[4 + op_len];
+						op_len += 5;
+					}
+				}
 
-			handled = 1;
+				switch (opcode.byte[2]) {
+				case 0x14:
+					INSTR_NAME("pextrb");
+					if (testREX(prefixREX, REX_W)) {
+						dstLength = 8;
+					}
+					else {
+						dstLength = 1;
+					}
+					extractValue= ssp_extract_epi8(&a, immValue);
+					break;
+				case 0x16:
+					if (testREX(prefixREX, REX_W)) {
+						INSTR_NAME("pextrq");
+						extractValue = ssp_extract_epi64(&a, immValue);
+						dstLength = 8;
+					}
+					else {
+						INSTR_NAME("pextrd");
+						extractValue = ssp_extract_epi32(&a, immValue);
+						dstLength = 4;
+					}
+					break;
+				case 0x17:
+					INSTR_NAME("extractps");
+					extractValue = ssp_extract_ps(&a, immValue);
+					dstLength = 4;
+					break;
+				}
 
-			switch (opcode.byte[3]) {
-			case 0xd1:
-				asm volatile("movdqa %%xmm2, %0" : "=m" (a));
-				asm volatile("movdqa %%xmm1, %0" : "=m" (b));
-				break;
-			case 0xec:
-				asm volatile("movdqa %%xmm5, %0" : "=m" (a));
-				asm volatile("movdqa %%xmm4, %0" : "=m" (b));
-				break;
-			case 0xe3:
-				asm volatile("movdqa %%xmm4, %0" : "=m" (a));
-				asm volatile("movdqa %%xmm3, %0" : "=m" (b));
-				break;
-			case 0xda:
-				asm volatile("movdqa %%xmm3, %0" : "=m" (a));
-				asm volatile("movdqa %%xmm2, %0" : "=m" (b));
-				break;
-			case 0xf1:
-				asm volatile("movdqa %%xmm6, %0" : "=m" (a));
-				asm volatile("movdqa %%xmm1, %0" : "=m" (b));
-				break;
-			case 0xd4:
-				asm volatile("movdqa %%xmm2, %0" : "=m" (a));
-				asm volatile("movdqa %%xmm4, %0" : "=m" (b));
-				break;
-			default:
-				handled = 0;
-				break;
+				if (memAddr && dstLength) {
+					handled = !copy_to_user((void __user *)memAddr, &extractValue, dstLength);
+				}
+				else if (dstLength) {
+					unsigned long *regPtr = getRegisterPtr(regIndex, regs, testREX(prefixREX, REX_B));
+					switch (dstLength) {
+					case 1:
+						*regPtr &= ~0xffUL;
+						*regPtr |= extractValue & 0xff;
+						handled = 1;
+						break;
+					case 4:
+						*regPtr &= ~0xffffffffUL;
+						*regPtr |= extractValue & 0xffffffff;
+						handled = 1;
+						break;
+					case 8:
+						*regPtr = extractValue;
+						handled = 1;
+						break;
+					}
+				}
 			}
 
-			ssp_alignr_epi8(&ret, &a, &b, ralign);
+			if (!handled && (op_len = getOp2XMMValue(opcode.byte[3], regs, prefixREX, &opcode.byte[4], &b)) != -1) {
+				immValue = opcode.byte[4 + op_len];
+				op_len += 5;
 
-			switch (opcode.byte[3]) {
-			case 0xd1:
-			case 0xd4:
-				asm volatile("movdqa %0, %%xmm2" : : "m" (ret));
-				break;
-			case 0xec:
-				asm volatile("movdqa %0, %%xmm5" : : "m" (ret));
-				break;
-			case 0xe3:
-				asm volatile("movdqa %0, %%xmm4" : : "m" (ret));
+				switch (opcode.byte[2]) {
+				case 0x08:
+					INSTR_NAME("roundps");
+					ret = ssp_round_ps(&b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x09:
+					INSTR_NAME("roundpd");
+					ret = ssp_round_pd(&b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x0a:
+					INSTR_NAME("roundss");
+					ret = ssp_round_ss(&a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x0b:
+					INSTR_NAME("roundsd");
+					ret = ssp_round_sd(&a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x0c:
+					INSTR_NAME("blendps");
+					ret = ssp_blend_ps(&a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x0d:
+					INSTR_NAME("blendpd");
+					ret = ssp_blend_pd(&a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x0e:
+					INSTR_NAME("pblendw");
+					ret = ssp_blend_epi16(&a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x0f:
+					INSTR_NAME("palignr");
+					ssp_alignr_epi8(&ret, &a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x21:
+					INSTR_NAME("insertps");
+					ret = ssp_insert_ps(&a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x40:
+					INSTR_NAME("dpps");
+					ret = ssp_dp_ps(&a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x41:
+					INSTR_NAME("dppd");
+					ret = ssp_dp_pd(&a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				case 0x42:
+					INSTR_NAME("mpsadbw");
+					ret = ssp_mpsadbw_epu8(&a, &b, immValue);
+					handled = 1;
+					setXMMRegister(aIndex, testREX(prefixREX, REX_R), &ret);
+					break;
+				}
+			}
+
+			if (handled) {
+				regs->ip += op_len;
+			}
+		}
+		else if (opcode.byte[1] == 0xb8 && opcode.byte[2] >= 0xc0) {
+			// popcnt with memory addressing not supported yet
+			unsigned int srcIndex = opcode.byte[2] & 0x7;
+			unsigned int dstIndex = (opcode.byte[2] >> 3) & 0x7;
+			int op_bytes = testREX(prefixREX, REX_W) ? 8 : (prefix66 ? 2 : 4);
+
+			unsigned long regValue = *getRegisterPtr(srcIndex, regs, testREX(prefixREX, REX_B));
+			unsigned long *dstReg = getRegisterPtr(dstIndex, regs, testREX(prefixREX, REX_R));
+
+			switch (op_bytes) {
+			case 2:
+				INSTR_NAME("popcnt.16");
+				*dstReg &= ~0xffffUL;
+				*dstReg |= ssp_popcnt_16(regValue);
 				break;
-			case 0xda:
-				asm volatile("movdqa %0, %%xmm3" : : "m" (ret));
+			case 4:
+				INSTR_NAME("popcnt.32");
+				*dstReg &= ~0xffffffffUL;
+				*dstReg |= ssp_popcnt_32(regValue);
 				break;
-			case 0xf1:
-				asm volatile("movdqa %0, %%xmm6" : : "m" (ret));
+			case 8:
+				INSTR_NAME("popcnt.64");
+				*dstReg = ssp_popcnt_64(regValue);
 				break;
 			}
-			regs->ip += 5;
+
+			handled = 1;
+			regs->ip += 3;
 		}
 	}
 
+#if DEBUG_INST_EMULATION
+	u8 buf[32];
+	copy_from_user((void *)buf, (const void __user *)(regs->ip - 16), sizeof(buf));
+	pr_info("invalid opcode %s %8llx %4x handled: %d REX: %#x %s\n", __instr_name ? __instr_name : "UNKNOWN",
+			swab64(*(u64*)&opcode.byte[0]), swab32(*(u32*)&opcode.byte[8]), handled, prefixREX, prefix66 ? "V" : "");
+	pr_info("code around ip: \n");
+	pr_info("%8llx %8llx %8llx %8llx\n", swab64(*(u64*)&buf[0]), swab64(*(u64*)&buf[8]),
+			swab64(*(u64*)&buf[16]), swab64(*(u64*)&buf[24]));
+#endif
+
 	if (!handled) {
 		if (notify_die(DIE_TRAP, "invalid opcode", regs, error_code,
 			X86_TRAP_UD, SIGILL) == NOTIFY_STOP) {
-- 
2.17.1

